[
["index.html", "1 Introduction", " 1 Introduction "],
["preface.html", "2 Preface", " 2 Preface We can start with the question: “in Milwaukee, is there a relationship between how many minorities are in a neigborhood, and how long it takes to fix pot holes there?” 2.0.1 The original story Add a link here 2.0.2 More stuff here? Blah blah "],
["turning-our-question-into-measureables.html", "2.1 Turning our question into measureables", " 2.1 Turning our question into measureables First, we’ll need to break this question down into something measureable, then figure out where we can get the necessary information. 2.1.1 Data: Potholes We’ll start with the time to fix pot holes. People report potholes, Milwaukee puts the reports into their system, and then the potholes get fixed. Let’s assume we can get this dataset, and that it will be in a reasonable form. To do our job, we’d probably want these three columns: A street address (to know where the pothole is) The time the pothole was reported The time the pothole was filled in A “time it took to fill in the pothole” column might be nice, but it’s kind of unrealistic! We can just compute that ourselves by subtracting the report vs. filled-in times. If you start searching around about potholes in Milwaukee, you quickly end up on this page about reporting a pot hole. These potholes are reported to - and fixed by - the Department of Public Works. According to the County of Milwaukee’s Open Records Request page: Each county department and elected official is the custodian of their respective records. As a result, each department and each elected official fulfills their own records requests. Therefore, to obtain the records you seek, you need to direct your open records requests to the appropriate record custodian. For example, if you are looking for pension information, Human Resources would be the custodian of those records. If you are looking for an accident report from an incident on the freeway, the Sheriff’s Department would be the custodian of those records. And so you might file a request with them (as I did!) and come away with a few CSV files with exactly the columns we asked for. import pandas as pd pd.set_option(&quot;display.max_columns&quot;, 20) pd.set_option(&quot;display.max_colwidth&quot;, 200) df = pd.read_excel(&quot;data/2007-2010 POTHOLES.xls&quot;) df.head(3) A Street EnterDt PrintDt ResolvDt 1846 W HALSEY AV 2010-07-15 16:33 2010-07-16 15:44 2010-07-19 15:14 9324 W PARK HILL AV 2010-07-15 16:06 2010-07-16 10:05 2010-07-21 06:02 1020 E MANITOBA ST 2010-07-15 15:13 2010-07-15 15:33 2010-07-16 14:35 Our dataset is all pothole requests filed between July 15 2007-July 15 2017. 2.1.2 Data: Number of minorities in a neighborhood This is a very, very, very vague statement, but it’s a fine starting point. The Census Bureau collects information on who lives where, and releases data each and very year through the American Community Survey, so it sounds like it’ll be a good resource. But neighborhoods? Unfortunately, they aren’t included on the Census. They aren’t included on the Census because in most places they’re vaguely defined. Instead, the census has a crazy set of different levels, including: Census blocks Block groups Census tracts ZIP code tabulation areas School districts Counties States …many, many more. I’ll give you a secret: most of the time, your answer is going to be census tracts. They’re pretty small - but not too small - and most types of data are available for them. If we use Social Explorer one of the first questions we have is what year do we want our data for? 2.1.2.1 Picking a year (editorial choice) Our pothole data is from 2007-2017, but I’m sure the city of Milwaukee has changed a lot during that time. Neighborhoods shift, people move in and out, and what an area was like in 2007 isn’t necessarily the same a decade later. We have three options: Pick a year in the middle and assume it will, on average, be like 2007 and 2017. Download census data for each year, matching up the years between the potholes and the census data, and combining them all together Pick one year and do it just for that one year. Picking an average year seems like the easiest way to get the most pothole data into our analysis, but it’s really not the most responsible method. If we have pothole data from each year and census data from each year, we’d only really be doing this because we’re too lazy to match up the years! It also isn’t very good from the perspective of finding interesting stories: imagine if a neighborhood were rapidly changing, with a lot of wealthy white people moving in, and suddenly potholes were filling much more quickly - wouldn’t you want to be able to notice that? Downloading every year of data is definitely the most thorough approach, but there could be a downside depending on how we do our analysis. If we study all ten years at once, we might miss changes over time, or things that happened in smaller windows. For example, what if in the past 3 years the Department of Public Works became much, much worse at filling potholes? We don’t want to miss that one! It’s also a lot of work! Picking just one year is a simple way to do the analysis, and allows you to expand to more years later on if you find anything interesting. Since we’re probably on a deadline, that’ll be our approach. We’re going to pick the year 2013 for this walkthrough. That’ll allow you to reproduce the original analysis for 2007 if you want, or something more recently - 2017 - if you want. TALK MORE ABOUT SELECTING A YEAR FLAG EDITORIAL DECISIONS TALK MORE ABOUT SELECTING A TABLE FLAG EDITORIAL DECISIONS TALK ABOUT READING THE DATA DICTIONARY 2.1.2.2 Picking a data table (editorial choice) Since we’re looking at “number of minorities,” our first instinct is to use Table A03001: Race. If we look at the data in the table, though, it breaks the population down into these categories: White Alone Black or African-American Alone American Indian or Alaska Native Alone Asian Alone Native Hawaiian and Other Pacific Islander Alone Some Other Race Alone Two or More Races Does this seem okay? It honestly depends on what you mean by “number of minorities” - is that just a way of saying “the Black population,” or “non-White people,” or something else altogether? One thing you might notice is that this table doesn’t include Hispanic/Latinx as a breakdown. According to the Census Bureau, people of Hispanic origin can be any race. There’s a lot of interesting history regarding how “Hispanic” wound up on the Census form and race on the census in general, but for now we’ll say we’re interested in that population, and we need to track it down. There’s another table, right after our “Race” table, called A04001. Hispanic or Latino by Race. It breaks people down in a similar way as the race table, but also includes whether they identify as Hispanic/Latino: Not Hispanic or Latino White Alone Black or African-American Alone American Indian or Alaska Native Alone Asian Alone Native Hawaiian and Other Pacific Islander Alone Some Other Race Alone Two or More Races Hispanic or Latino White Alone Black or African-American Alone American Indian or Alaska Native Alone Asian Alone Native Hawaiian and Other Pacific Islander Alone Some Other Race Alone Two or More Races This table seems a lot more useful if we’re looking for a definition of “minority” that includes Hispanic/Latinx populations. With this dataset, we’ll be examining non-Hispanic White Alone as white, and everyone else as a minority. 2.1.2.3 Downloading our data When you download your data, make sure you scroll to the bottom and get the Data Dictionary. The dataset itself is full of weird codes, and the data dictionary will enable us to understand them. "],
["combining-our-datasets.html", "2.2 Combining our datasets", " 2.2 Combining our datasets The Milwaukee Department of Public Works can’t create one Excel file big enough to hold ten years of records, so they sent the data to us in three separate files: 2007-2010 POTHOLES.xls 2010-2013 POTHOLES.xls 2013-2017 POTHOLES.xls First, we’re going to open them each up and confirm their columns are all the same. # Open up 2007-2010 df_2007 = pd.read_excel(&quot;data/2007-2010 POTHOLES.xls&quot;) df_2007.head(3) A Street EnterDt PrintDt ResolvDt 1846 W HALSEY AV 2010-07-15 16:33 2010-07-16 15:44 2010-07-19 15:14 9324 W PARK HILL AV 2010-07-15 16:06 2010-07-16 10:05 2010-07-21 06:02 1020 E MANITOBA ST 2010-07-15 15:13 2010-07-15 15:33 2010-07-16 14:35 # Open up 2010-2013 df_2010 = pd.read_excel(&quot;data/2010-2013 POTHOLES.xls&quot;) df_2010.head(3) A Street EnterDt PrintDt ResolvDt 3839 N 10TH ST 2013-07-15 23:35 2013-07-16 05:46 2013-07-17 05:50 4900 W MELVINA ST 2013-07-15 20:05 2013-07-16 05:46 2013-07-24 16:58 2400 W WISCONSIN AV 2013-07-15 20:00 2013-07-16 05:56 2013-07-25 14:42 # Open up 2013-2017 df_2013 = pd.read_excel(&quot;data/2013-2017 POTHOLES.xls&quot;) df_2013.head(3) A Street EnterDt PrintDt ResolvDt 7741 N 59TH ST 2017-07-15 08:55 2017-07-17 05:33 2017-07-21 04:51 5517 N 39TH ST 2017-07-14 22:36 2017-07-17 05:33 2017-07-25 15:29 8242 N GRANVILLE RD 2017-07-14 18:30 2017-07-17 05:33 2017-07-17 06:55 Looks good! Matching columns, and they seem to start at the right date. Let’s do an extra check to make sure the files don’t have any overlap - it’d be no good if 2007-2010 and 2010-2013 both included 2010 and we ended up double-counting the year. # min/max of the date in 2007-2010 df_2007.agg({&#39;EnterDt&#39;: [&#39;min&#39;, &#39;max&#39;]}) ## EnterDt ## min 2007-07-16 07:53 ## max 2010-07-15 16:33 # min/max of the date in 2010-2013 df_2010.agg({&#39;EnterDt&#39;: [&#39;min&#39;, &#39;max&#39;]}) ## EnterDt ## min 2010-07-16 06:34 ## max 2013-07-15 23:35 # min/max of the date in 2013-2017 df_2013.agg({&#39;EnterDt&#39;: [&#39;min&#39;, &#39;max&#39;]}) ## EnterDt ## min 2013-07-16 05:40 ## max 2017-07-15 08:55 Okay, we’re looking pretty good! Let’s combine the three dataframes into a single dataframe. potholes = pd.concat([ df_2007, df_2010, df_2013 ]) potholes.shape ## (120186, 5) Now we’re looking at 120,186 rows of potholes across those ten years. Also, a little more cleaning. See anything that feels weird about the columns in our dataset? potholes.head() A Street EnterDt PrintDt ResolvDt 0 1846 W HALSEY AV 2010-07-15 16:33 2010-07-16 15:44 2010-07-19 15:14 1 9324 W PARK HILL AV 2010-07-15 16:06 2010-07-16 10:05 2010-07-21 06:02 2 1020 E MANITOBA ST 2010-07-15 15:13 2010-07-15 15:33 2010-07-16 14:35 3 3200 S 72ND ST 2010-07-15 15:12 2010-07-15 15:33 2010-07-16 14:36 4 6001 W WARNIMONT AV 2010-07-15 15:11 2010-07-15 15:33 2010-07-16 14:35 Instead of a single address column, we have a street - Street - and a house number - A. It makes sense to combine those to get a new column that’s the whole address, yeah? # Address number read in as a number, so we need to convert it to a string potholes[&#39;address&#39;] = potholes.A.astype(str) + &quot; &quot; + potholes.Street potholes = potholes.drop(columns=[&#39;A&#39;, &#39;Street&#39;]) potholes.head() EnterDt PrintDt ResolvDt address 0 2010-07-15 16:33 2010-07-16 15:44 2010-07-19 15:14 1846 W HALSEY AV 1 2010-07-15 16:06 2010-07-16 10:05 2010-07-21 06:02 9324 W PARK HILL AV 2 2010-07-15 15:13 2010-07-15 15:33 2010-07-16 14:35 1020 E MANITOBA ST 3 2010-07-15 15:12 2010-07-15 15:33 2010-07-16 14:36 3200 S 72ND ST 4 2010-07-15 15:11 2010-07-15 15:33 2010-07-16 14:35 6001 W WARNIMONT AV Now that our data is nice and clean we can save it. We might have to send it to someone else at some point, and we wouldn’t want to do all this cleaning again, right? potholes.to_csv(&quot;data/potholes-merged.csv&quot;, index=False) 2.2.1 Merging Census data and pothole data If we look at our Census dataset, we see a couple columns that identify the census tract: Geo_FIPS and Geo_NAME. census = pd.read_csv(&quot;data/R12216099_SL140.csv&quot;, dtype={&#39;Geo_FIPS&#39;: &#39;str&#39;}) census.head(2) Geo_FIPS Geo_GEOID Geo_NAME Geo_QName Geo_STUSAB Geo_SUMLEV Geo_GEOCOMP Geo_FILEID Geo_LOGRECNO Geo_US Geo_REGION Geo_DIVISION Geo_STATECE Geo_STATE Geo_COUNTY Geo_COUSUB Geo_PLACE Geo_PLACESE Geo_TRACT Geo_BLKGRP Geo_CONCIT Geo_AIANHH Geo_AIANHHFP Geo_AIHHTLI Geo_AITSCE Geo_AITS Geo_ANRC Geo_CBSA Geo_CSA Geo_METDIV Geo_MACC Geo_MEMI Geo_NECTA Geo_CNECTA Geo_NECTADIV Geo_UA Geo_UACP Geo_CDCURR Geo_SLDU Geo_SLDL Geo_VTD Geo_ZCTA3 Geo_ZCTA5 Geo_SUBMCD Geo_SDELM Geo_SDSEC Geo_SDUNI Geo_UR Geo_PCI Geo_TAZ Geo_UGA Geo_BTTR Geo_BTBG Geo_PUMA5 Geo_PUMA1 SE_A04001_001 SE_A04001_002 SE_A04001_003 SE_A04001_004 SE_A04001_005 SE_A04001_006 SE_A04001_007 SE_A04001_008 SE_A04001_009 SE_A04001_010 SE_A04001_011 SE_A04001_012 SE_A04001_013 SE_A04001_014 SE_A04001_015 SE_A04001_016 SE_A04001_017 55079000101 14000US55079000101 Census Tract 1.01, Milwaukee County, Wisconsin Census Tract 1.01, Milwaukee County, Wisconsin wi 140 0 ACSSF 4717 NaN NaN NaN NaN 55 79 NaN NaN NaN 101 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 5306 5015 1343 3567 13 70 0 16 6 291 37 0 0 0 0 254 0 55079000102 14000US55079000102 Census Tract 1.02, Milwaukee County, Wisconsin Census Tract 1.02, Milwaukee County, Wisconsin wi 140 0 ACSSF 4718 NaN NaN NaN NaN 55 79 NaN NaN NaN 102 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 3648 3549 1034 2195 36 118 0 0 166 99 67 32 0 0 0 0 0 We had to read in Geo_FIPS as a string, otherwise pandas would convert it into a very long number. If we look at our pothole dataset, can we match any of the columns with either of those columns? potholes = pd.read_csv(&quot;data/potholes-merged.csv&quot;) potholes.head(2) EnterDt PrintDt ResolvDt address 2010-07-15 16:33 2010-07-16 15:44 2010-07-19 15:14 1846 W HALSEY AV 2010-07-15 16:06 2010-07-16 10:05 2010-07-21 06:02 9324 W PARK HILL AV Unfortunately not! Even though we have the street address, which we think of as the “location,” we can’t join it onto the Census dataset because we need a matching column. What we need is to add a new column that includes the census tract name or number, which means we need to find out which addresses are inside which census tracts. While there might be a service we can feed the address to and get the census tract back, we’re going to do something a little more generally useful! From the Census Bureau, we can get a geographic file (a shapefile) of where each census tract is. If we can put the street addresses on the same map, we’ll be able to say “find out which shape you’re in, and take the census tract ID from that shape.” VISUAL THING OF THAT But how do we put the street addresses on a map? For that, we need to convert their street address to latitude and longitude, a process called geocoding. Geocoding (or georeferencing) does just that: converts a “normal” way of describing a place, like a street address or a ZIP code or a city name, and converts it into latitude and longitude. For most geocoding services we’re going to be paying per address. I’m cheap, so let’s look at how many rows we have: potholes.shape ## (120186, 4) Oof, look at that! Geocoding 100,000 addresses is going to cost more than we want to pay - are there any shortcuts we can think of to geocoding all these pothole addresses? Luckily, there is! Each row isn’t actually an address - it’s a pothole at an address. This means the same address might show up in our dataset multiple times. Let’s take a look: potholes.address.value_counts().head(10) x 4700 S HOWELL AV 84 6000 W HAMPTON AV 77 6000 W OKLAHOMA AV 74 2700 S 60TH ST 73 6000 W FOND DU LAC AV 72 7400 W APPLETON AV 71 2700 W MORGAN AV 70 7600 W HAMPTON AV 65 10700 W GOOD HOPE RD 63 3100 S 60TH ST 60 Yowza, that’s an impressive number of potholes at one address! It also means we’re lucky! Instead of geocoding each row individually - which would mean geocoding 4700 S HOWELL AV 84 times - we can pull out the addresses, remove the duplicates, geocode them, and then join them back into our dataset. How many unique addresses do we have? # Unique list of addresses unique_addresses = potholes.address.unique() len(unique_addresses) ## 53172 Wow, that’s about a 50% savings from the original list! Let’s put it into a dataframe to make the unique addresses easier to work with. address_df = pd.DataFrame({ &#39;address&#39;: unique_addresses, &#39;city&#39;: &#39;Milwaukee&#39;, &#39;state&#39;: &#39;Wisconsin&#39; }) address_df.head() address city state 1846 W HALSEY AV Milwaukee Wisconsin 9324 W PARK HILL AV Milwaukee Wisconsin 1020 E MANITOBA ST Milwaukee Wisconsin 3200 S 72ND ST Milwaukee Wisconsin 6001 W WARNIMONT AV Milwaukee Wisconsin I added the city and state because depending on what you use to geocode, it might come in handy. Let’s save this list of addresses in case we need the separate file. address_df.to_csv(&quot;data/addresses.csv&quot;, index=False) 2.2.2 Performing the geocoding While I wrote some code to do the geocoding in Python with Google’s API, I ended up using Geocod.io instead because it’s a really, really, really easy service, and Google would cost over twice as much! You just drag and drop the CSV file and it emails you when it’s finished. Geocoding this dataset of 50k addresses cost me about $15., and Geocod.io even offers to add on census information to our geocoded addresses for an extra fee. I didn’t do that because we’re going to do it for free in a minute. When the geocoding was completed, I saved the new dataset as addresses_geocoded.csv. "],
["attaching-tract-data-to-our-latitudes-and-longitudes.html", "2.3 Attaching tract data to our latitudes and longitudes", " 2.3 Attaching tract data to our latitudes and longitudes Now that we know where our street addresses are in space, we need to figure out what census tract they’re in. Or, more specifically, which census tract they were in as of 2013. We’ll be using geographic information from the US Census Bureau, specifically its TIGER/Line Shapefiles product. If you aren’t familiar with shapefiles, they’re more or less a data format for geographic information that you can open up in applications like QGIS (which is what we’ll be doing our work in). Each year the Census Bureau releases a new set of shapefiles, and they have also produced detailed documentation about what set of shapefiles to use depending on where/when your data came from. Our data came from the 2013 American Community Survey (5-year, 2007-2013), so we’ll be using the 2013 shapefiles. Specifically the ones for at the census tract level in Wisconsin. 2.3.1 Joining attributes by location JOINING ATTRIBUTES BY LOCATION After we do our join, we can see that our new Attribute Table lists all sorts of geographic data along with our original information. To save this to a CSV, right-click and select Export as…. Pick Comma Separated Values (CSV) from the filetype menu, and save it as 2013-addresses-tracts.csv. We specifically call it 2013 because even though these addresses are from the entire dataset, this is the census tract they were in during 2013. Why didn’t we grab addresses from the entire dataset instead of just addresses with 2013 potholes? I thought we might need the other addresses later in case we decide to analyze the other years, so it seems reasonable to do them all at once. Now that we have this CSV of addresses, we can open them in pandas like so: tracts = pd.read_csv(&quot;data/2013-addresses-tracts.csv&quot;) tracts.head() address city state Latitude Longitude Accuracy Score Accuracy Type Number Street City_1 State_1 County Zip Country Source GEOID 8900 N 124TH ST Milwaukee Wisconsin 43.17790 -88.06365 1 range_interpolation 8900 N 124th St Milwaukee WI Waukesha County 53224 US TIGER/Line® dataset from the US Census Bureau 55133200202 2200 N TERRACE AV Milwaukee Wisconsin 43.05671 -87.88080 1 rooftop 2200 N Terrace Ave Milwaukee WI Milwaukee County 53202 US City of Milwaukee 55079186900 1000 N PROSPECT AV Milwaukee Wisconsin 43.04328 -87.89869 1 range_interpolation 1000 N Prospect Ave Milwaukee WI Milwaukee County 53202 US TIGER/Line® dataset from the US Census Bureau 55079186900 1700 N PROSPECT AV Milwaukee Wisconsin 43.05211 -87.89049 1 rooftop 1700 N Prospect Ave Milwaukee WI Milwaukee County 53202 US City of Milwaukee 55079186900 2298 N TERRACE AV Milwaukee Wisconsin 43.05931 -87.87922 1 range_interpolation 2298 N Terrace Ave Milwaukee WI Milwaukee County 53202 US TIGER/Line® dataset from the US Census Bureau 55079186900 Lots of extra columns there, so when we read it in later we’ll probably only take a few of the columns. "],
["merging-our-datasets.html", "2.4 Merging our datasets", " 2.4 Merging our datasets Let’s review the data that we have: A csv file of addresses and pothole report/fill times A csv file of addresses and geographic information, including census tract identifiers A csv file of census tract identifiers and race data We’ll need to do a few merges in order to complete our finished dataset. 2.4.0.1 Our base dataset: potholes Let’s start with our pothole dataset. Right after we read it in we’re going to filter for 2013. That’s pretty easy to do because pandas read the date in as text! potholes = pd.read_csv(&quot;data/potholes-merged.csv&quot;) potholes = potholes[potholes.EnterDt.str.startswith(&#39;2013&#39;)] potholes.head() EnterDt PrintDt ResolvDt address 38113 2013-07-15 23:35 2013-07-16 05:46 2013-07-17 05:50 3839 N 10TH ST 38114 2013-07-15 20:05 2013-07-16 05:46 2013-07-24 16:58 4900 W MELVINA ST 38115 2013-07-15 20:00 2013-07-16 05:56 2013-07-25 14:42 2400 W WISCONSIN AV 38116 2013-07-15 19:55 2013-07-16 05:46 2013-07-18 06:06 1800 W HAMPTON AV 38117 2013-07-15 19:50 2013-07-16 05:46 2013-08-02 06:08 4718 N 19TH ST 2.4.0.2 Merging pothole addresses with tracts To give these addresses census tract identifiers, we’ll join it to our geocoded tract dataset. As I mentioned before, we’re only interested in a couple of the columns - address and tract - so we’ll ignore the lat/lon, city, state, and all of that. tracts = pd.read_csv(&quot;data/2013-addresses-tracts.csv&quot;, dtype={&#39;GEOID&#39;: &#39;str&#39;}, usecols=[&#39;address&#39;, &#39;GEOID&#39;]) tracts.head() address GEOID 8900 N 124TH ST 55133200202 2200 N TERRACE AV 55079186900 1000 N PROSPECT AV 55079186900 1700 N PROSPECT AV 55079186900 2298 N TERRACE AV 55079186900 Any time you read in a geographic identifier - ZIP codes, etc - you’ll want to read them in as strings. Since they look like numbers, pandas would take a ZIP code like 06002 and turn it into the integer 6002. To prevent any bad situations arising from something like that, we preemptively read in all of our geographic codes as strings. Now we can join them together based on the address, giving each pothole report a census tract. Since they both have the same column name for the address - address - we’ll be able to just use on='address'. potholes_tracts = potholes.merge(tracts, on=&#39;address&#39;, how=&#39;left&#39;) potholes_tracts.head() EnterDt PrintDt ResolvDt address GEOID 0 2013-07-15 23:35 2013-07-16 05:46 2013-07-17 05:50 3839 N 10TH ST 55079004500 1 2013-07-15 20:05 2013-07-16 05:46 2013-07-24 16:58 4900 W MELVINA ST 55079003800 2 2013-07-15 20:00 2013-07-16 05:56 2013-07-25 14:42 2400 W WISCONSIN AV 55079014900 3 2013-07-15 19:55 2013-07-16 05:46 2013-07-18 06:06 1800 W HAMPTON AV 55079002300 4 2013-07-15 19:50 2013-07-16 05:46 2013-08-02 06:08 4718 N 19TH ST 55079002300 Usually when there’s no match, pandas drops the row from the merged dataset. We add how='left' so that any row that isn’t in our geocoded dataset is still in there. It’s good to be able to check and see if you’re missing anything! How many are missing census tracts? potholes_tracts.GEOID.isna().value_counts() ## False 12822 ## True 14 ## Name: GEOID, dtype: int64 2.4.0.3 Merging potholes with census data Now we can read in our census data. Again, we’ll be sure to treat the census tract number as a string to prevent any merging issues. census = pd.read_csv(&quot;data/R12216099_SL140.csv&quot;, dtype={&#39;Geo_FIPS&#39;: &#39;str&#39;}) census.head() Geo_FIPS Geo_GEOID Geo_NAME Geo_QName Geo_STUSAB Geo_SUMLEV Geo_GEOCOMP Geo_FILEID Geo_LOGRECNO Geo_US Geo_REGION Geo_DIVISION Geo_STATECE Geo_STATE Geo_COUNTY Geo_COUSUB Geo_PLACE Geo_PLACESE Geo_TRACT Geo_BLKGRP Geo_CONCIT Geo_AIANHH Geo_AIANHHFP Geo_AIHHTLI Geo_AITSCE Geo_AITS Geo_ANRC Geo_CBSA Geo_CSA Geo_METDIV Geo_MACC Geo_MEMI Geo_NECTA Geo_CNECTA Geo_NECTADIV Geo_UA Geo_UACP Geo_CDCURR Geo_SLDU Geo_SLDL Geo_VTD Geo_ZCTA3 Geo_ZCTA5 Geo_SUBMCD Geo_SDELM Geo_SDSEC Geo_SDUNI Geo_UR Geo_PCI Geo_TAZ Geo_UGA Geo_BTTR Geo_BTBG Geo_PUMA5 Geo_PUMA1 SE_A04001_001 SE_A04001_002 SE_A04001_003 SE_A04001_004 SE_A04001_005 SE_A04001_006 SE_A04001_007 SE_A04001_008 SE_A04001_009 SE_A04001_010 SE_A04001_011 SE_A04001_012 SE_A04001_013 SE_A04001_014 SE_A04001_015 SE_A04001_016 SE_A04001_017 55079000101 14000US55079000101 Census Tract 1.01, Milwaukee County, Wisconsin Census Tract 1.01, Milwaukee County, Wisconsin wi 140 0 ACSSF 4717 NaN NaN NaN NaN 55 79 NaN NaN NaN 101 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 5306 5015 1343 3567 13 70 0 16 6 291 37 0 0 0 0 254 0 55079000102 14000US55079000102 Census Tract 1.02, Milwaukee County, Wisconsin Census Tract 1.02, Milwaukee County, Wisconsin wi 140 0 ACSSF 4718 NaN NaN NaN NaN 55 79 NaN NaN NaN 102 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 3648 3549 1034 2195 36 118 0 0 166 99 67 32 0 0 0 0 0 55079000201 14000US55079000201 Census Tract 2.01, Milwaukee County, Wisconsin Census Tract 2.01, Milwaukee County, Wisconsin wi 140 0 ACSSF 4719 NaN NaN NaN NaN 55 79 NaN NaN NaN 201 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 4791 4019 1185 2657 0 102 0 0 75 772 120 0 0 0 0 613 39 55079000202 14000US55079000202 Census Tract 2.02, Milwaukee County, Wisconsin Census Tract 2.02, Milwaukee County, Wisconsin wi 140 0 ACSSF 4720 NaN NaN NaN NaN 55 79 NaN NaN NaN 202 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 6324 6148 2687 2810 87 390 12 0 162 176 113 0 0 0 0 63 0 55079000301 14000US55079000301 Census Tract 3.01, Milwaukee County, Wisconsin Census Tract 3.01, Milwaukee County, Wisconsin wi 140 0 ACSSF 4721 NaN NaN NaN NaN 55 79 NaN NaN NaN 301 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 1339 1339 1005 220 0 90 0 10 14 0 0 0 0 0 0 0 0 Oh boy, how many columns is that? census.shape ## (298, 72) I count that as too many! Let’s drop all of the geographic columns we don’t need, which means everything between Geo_GEOID and Geo_PUMA1. Geo_GEOID looks like it would match the census tract column in our potholes dataset — GEOID — but in reality Geo_FIPS is the column we’ll be merging on (Geo_GEOID is the longer, national version). census = census.drop(columns=census.columns.to_series()[&quot;Geo_GEOID&quot;:&quot;Geo_PUMA1&quot;]) census.head() Geo_FIPS SE_A04001_001 SE_A04001_002 SE_A04001_003 SE_A04001_004 SE_A04001_005 SE_A04001_006 SE_A04001_007 SE_A04001_008 SE_A04001_009 SE_A04001_010 SE_A04001_011 SE_A04001_012 SE_A04001_013 SE_A04001_014 SE_A04001_015 SE_A04001_016 SE_A04001_017 55079000101 5306 5015 1343 3567 13 70 0 16 6 291 37 0 0 0 0 254 0 55079000102 3648 3549 1034 2195 36 118 0 0 166 99 67 32 0 0 0 0 0 55079000201 4791 4019 1185 2657 0 102 0 0 75 772 120 0 0 0 0 613 39 55079000202 6324 6148 2687 2810 87 390 12 0 162 176 113 0 0 0 0 63 0 55079000301 1339 1339 1005 220 0 90 0 10 14 0 0 0 0 0 0 0 0 We’ll keep all the numeric data for now because we haven’t really discussed the analysis yet. Now we can merge, and add the census data to each row of our potholes dataset. This time the column names we’re merging on are different for each dataset - GEOID for the potholes, Geo_FIPS for the census data - so we’ll need to use left_on= and right_on=. merged = potholes_tracts.merge(census, left_on=&#39;GEOID&#39;, right_on=&#39;Geo_FIPS&#39;, how=&#39;left&#39;) merged.head() EnterDt PrintDt ResolvDt address GEOID Geo_FIPS SE_A04001_001 SE_A04001_002 SE_A04001_003 SE_A04001_004 SE_A04001_005 SE_A04001_006 SE_A04001_007 SE_A04001_008 SE_A04001_009 SE_A04001_010 SE_A04001_011 SE_A04001_012 SE_A04001_013 SE_A04001_014 SE_A04001_015 SE_A04001_016 SE_A04001_017 0 2013-07-15 23:35 2013-07-16 05:46 2013-07-17 05:50 3839 N 10TH ST 55079004500 55079004500 3160 3053 76 2854 1 30 0 8 84 107 16 17 0 0 0 74 0 1 2013-07-15 20:05 2013-07-16 05:46 2013-07-24 16:58 4900 W MELVINA ST 55079003800 55079003800 2323 2276 205 2007 0 0 0 0 64 47 0 0 0 0 0 47 0 2 2013-07-15 20:00 2013-07-16 05:56 2013-07-25 14:42 2400 W WISCONSIN AV 55079014900 55079014900 1275 1113 514 538 0 50 0 0 11 162 148 3 0 0 0 11 0 3 2013-07-15 19:55 2013-07-16 05:46 2013-07-18 06:06 1800 W HAMPTON AV 55079002300 55079002300 4078 4063 179 3765 0 38 0 0 81 15 8 0 0 0 0 0 7 4 2013-07-15 19:50 2013-07-16 05:46 2013-08-02 06:08 4718 N 19TH ST 55079002300 55079002300 4078 4063 179 3765 0 38 0 0 81 15 8 0 0 0 0 0 7 Again, we’ve done how='left' so we can check on what census tracts we don’t have data for. SE_A04001_001 is the column for “Total Population,” so we can use it to see how many rows didn’t have that information added in. merged.SE_A04001_001.isna().value_counts() ## False 12820 ## True 16 ## Name: SE_A04001_001, dtype: int64 I’m impressed, that’s not very many at all! Now that our dataset is ready for analysis, let’s save it and move on to the next step. merged.to_csv(&quot;data/2013-complete.csv&quot;, index=False) "],
["final-prep-for-analysis.html", "2.5 Final prep for analysis", " 2.5 Final prep for analysis Let’s go back to our question: “in Milwaukee, is there a relationship between how many minorities are in a neigborhood, and how long it takes to fix potholes there?” We have two things we’re measuring: The amount of minorities How long it takes to fix potholes Before we do our analysis, we’ll need to see how (or if) our dataset is able to represent these concepts. Before we dive in, let’s read in our final (?) merged dataset and take a look at what we have. df = pd.read_csv(&quot;data/2013-complete.csv&quot;) df.head() EnterDt PrintDt ResolvDt address GEOID Geo_FIPS SE_A04001_001 SE_A04001_002 SE_A04001_003 SE_A04001_004 SE_A04001_005 SE_A04001_006 SE_A04001_007 SE_A04001_008 SE_A04001_009 SE_A04001_010 SE_A04001_011 SE_A04001_012 SE_A04001_013 SE_A04001_014 SE_A04001_015 SE_A04001_016 SE_A04001_017 2013-07-15 23:35 2013-07-16 05:46 2013-07-17 05:50 3839 N 10TH ST 55079004500 55079004500 3160 3053 76 2854 1 30 0 8 84 107 16 17 0 0 0 74 0 2013-07-15 20:05 2013-07-16 05:46 2013-07-24 16:58 4900 W MELVINA ST 55079003800 55079003800 2323 2276 205 2007 0 0 0 0 64 47 0 0 0 0 0 47 0 2013-07-15 20:00 2013-07-16 05:56 2013-07-25 14:42 2400 W WISCONSIN AV 55079014900 55079014900 1275 1113 514 538 0 50 0 0 11 162 148 3 0 0 0 11 0 2013-07-15 19:55 2013-07-16 05:46 2013-07-18 06:06 1800 W HAMPTON AV 55079002300 55079002300 4078 4063 179 3765 0 38 0 0 81 15 8 0 0 0 0 0 7 2013-07-15 19:50 2013-07-16 05:46 2013-08-02 06:08 4718 N 19TH ST 55079002300 55079002300 4078 4063 179 3765 0 38 0 0 81 15 8 0 0 0 0 0 7 A lot of those columns are weird census codes, but we’ll let them stay for now - we can always look them up in the census table’s data dictionary later. 2.5.1 “The amount of minorities” First, what counts as “minorities?” We decided that everyone whose race is not White - Asian, African-American, etc - gets counted as a minority, along with everyone who is marked as Hispanic/Latino, even if they’re White. So we’re looking specifically for White non-Hispanics. If we open up the data dictionary - R12217898.txt - we can see that A04001_003 is the column of our White Non-Hispanics. In fact it’s SE_A04001_003 thanks to Social Explorer, but you get the idea. Now, in our sentence we said “the amount of minorities” - but is this raw number really what we want? No way! We want a percentage of minorities. We could add up each and every one of those other columns, or we could just… calculate the percent of non-Hispanic Whites instead. It’s easier, right? According to our data dictionary A04001_001 is our total population and A04001_003 is our non-Hispanic white population, so simple division will get us what we need. After the computation, we can drop all of the other columns that came along with the census. # Calculate df[&#39;pct_white&#39;] = df.SE_A04001_003 / df.SE_A04001_001 # Drop the extra columns se_columns = df.columns[df.columns.str.contains(&quot;SE_&quot;)] df = df.drop(columns=se_columns) df.head() EnterDt PrintDt ResolvDt address GEOID Geo_FIPS pct_white 2013-07-15 23:35 2013-07-16 05:46 2013-07-17 05:50 3839 N 10TH ST 55079004500 55079004500 0.0240506 2013-07-15 20:05 2013-07-16 05:46 2013-07-24 16:58 4900 W MELVINA ST 55079003800 55079003800 0.0882480 2013-07-15 20:00 2013-07-16 05:56 2013-07-25 14:42 2400 W WISCONSIN AV 55079014900 55079014900 0.4031373 2013-07-15 19:55 2013-07-16 05:46 2013-07-18 06:06 1800 W HAMPTON AV 55079002300 55079002300 0.0438941 2013-07-15 19:50 2013-07-16 05:46 2013-08-02 06:08 4718 N 19TH ST 55079002300 55079002300 0.0438941 This calculation gives us a number 0-1 for percent of non-Hispanic Whites, so 0.25 would be 25% and 0.75 would be 75%. Nothing too crazy, and we’ll leave it alone for now. If we really want the percent of minorities, we can just subtract the percent white from 1. We might use it later, so let’s do that calculation now. df[&#39;pct_minority&#39;] = 1 - df.pct_white df.head() EnterDt PrintDt ResolvDt address GEOID Geo_FIPS pct_white pct_minority 2013-07-15 23:35 2013-07-16 05:46 2013-07-17 05:50 3839 N 10TH ST 55079004500 55079004500 0.0240506 0.9759494 2013-07-15 20:05 2013-07-16 05:46 2013-07-24 16:58 4900 W MELVINA ST 55079003800 55079003800 0.0882480 0.9117520 2013-07-15 20:00 2013-07-16 05:56 2013-07-25 14:42 2400 W WISCONSIN AV 55079014900 55079014900 0.4031373 0.5968627 2013-07-15 19:55 2013-07-16 05:46 2013-07-18 06:06 1800 W HAMPTON AV 55079002300 55079002300 0.0438941 0.9561059 2013-07-15 19:50 2013-07-16 05:46 2013-08-02 06:08 4718 N 19TH ST 55079002300 55079002300 0.0438941 0.9561059 2.5.2 “How long it takes to fix potholes” The time it takes to fix potholes seems pretty easy - it’s just the difference between when it got filed and when it got fixed, right? First we’ll convert the columns to datetimes, then we’ll subtract them. df[&#39;EnterDt&#39;] = pd.to_datetime(df.EnterDt) df[&#39;ResolvDt&#39;] = pd.to_datetime(df.ResolvDt) df[&#39;wait_time&#39;] = df.ResolvDt - df.EnterDt df[[&#39;EnterDt&#39;, &#39;ResolvDt&#39;, &#39;wait_time&#39;]].head() ## EnterDt ResolvDt wait_time ## 0 2013-07-15 23:35:00 2013-07-17 05:50:00 1 days 06:15:00 ## 1 2013-07-15 20:05:00 2013-07-24 16:58:00 8 days 20:53:00 ## 2 2013-07-15 20:00:00 2013-07-25 14:42:00 9 days 18:42:00 ## 3 2013-07-15 19:55:00 2013-07-18 06:06:00 2 days 10:11:00 ## 4 2013-07-15 19:50:00 2013-08-02 06:08:00 17 days 10:18:00 The problem in this case isn’t the result - it looks fine - it’s that we actually need a number. So we need to convert “1 days, 6 hours, 4 minutes and 34 seconds” into some sort of decimal. Besides the technical question, there’s an emotional one: do we talk about this in terms of hours, or in terms of days? It seems to me that saying “It took 2 days longer to fix a pothole” sounds more natural than “it took 48 hours longer,” so let’s go with days. df[&#39;wait_days&#39;] = df.wait_time.dt.days df.head(5) ## EnterDt PrintDt ResolvDt \\ ## 0 2013-07-15 23:35:00 2013-07-16 05:46 2013-07-17 05:50:00 ## 1 2013-07-15 20:05:00 2013-07-16 05:46 2013-07-24 16:58:00 ## 2 2013-07-15 20:00:00 2013-07-16 05:56 2013-07-25 14:42:00 ## 3 2013-07-15 19:55:00 2013-07-16 05:46 2013-07-18 06:06:00 ## 4 2013-07-15 19:50:00 2013-07-16 05:46 2013-08-02 06:08:00 ## ## address GEOID Geo_FIPS pct_white pct_minority \\ ## 0 3839 N 10TH ST 5.507900e+10 5.507900e+10 0.024051 0.975949 ## 1 4900 W MELVINA ST 5.507900e+10 5.507900e+10 0.088248 0.911752 ## 2 2400 W WISCONSIN AV 5.507901e+10 5.507901e+10 0.403137 0.596863 ## 3 1800 W HAMPTON AV 5.507900e+10 5.507900e+10 0.043894 0.956106 ## 4 4718 N 19TH ST 5.507900e+10 5.507900e+10 0.043894 0.956106 ## ## wait_time wait_days ## 0 1 days 06:15:00 1.0 ## 1 8 days 20:53:00 8.0 ## 2 9 days 18:42:00 9.0 ## 3 2 days 10:11:00 2.0 ## 4 17 days 10:18:00 17.0 The problem is that we’re really missing all of the time between days here, and it’s all whole numbers - 3 days, 5 days, etc. Fractions of days might be a little better, but it means we can’t use the nice and friendly .dt.days. Instaed we’ll need to use .dt.components['days'] and the like, which is a little slower. # Hours get divided by 24, so 12 hours is 0.5 days df[&#39;wait_days&#39;] = df.wait_time.dt.components[&#39;days&#39;] + (df.wait_time.dt.components[&#39;hours&#39;] / 24) df.head(5) ## EnterDt PrintDt ResolvDt \\ ## 0 2013-07-15 23:35:00 2013-07-16 05:46 2013-07-17 05:50:00 ## 1 2013-07-15 20:05:00 2013-07-16 05:46 2013-07-24 16:58:00 ## 2 2013-07-15 20:00:00 2013-07-16 05:56 2013-07-25 14:42:00 ## 3 2013-07-15 19:55:00 2013-07-16 05:46 2013-07-18 06:06:00 ## 4 2013-07-15 19:50:00 2013-07-16 05:46 2013-08-02 06:08:00 ## ## address GEOID Geo_FIPS pct_white pct_minority \\ ## 0 3839 N 10TH ST 5.507900e+10 5.507900e+10 0.024051 0.975949 ## 1 4900 W MELVINA ST 5.507900e+10 5.507900e+10 0.088248 0.911752 ## 2 2400 W WISCONSIN AV 5.507901e+10 5.507901e+10 0.403137 0.596863 ## 3 1800 W HAMPTON AV 5.507900e+10 5.507900e+10 0.043894 0.956106 ## 4 4718 N 19TH ST 5.507900e+10 5.507900e+10 0.043894 0.956106 ## ## wait_time wait_days ## 0 1 days 06:15:00 1.250000 ## 1 8 days 20:53:00 8.833333 ## 2 9 days 18:42:00 9.750000 ## 3 2 days 10:11:00 2.416667 ## 4 17 days 10:18:00 17.416667 Now we have our dataset ready for analysis, with each row having a time that it took to fix the potholes and a number describining the demographics of that census tract. 2.5.3 Removing missing data When we’re doing our final analysis, we want to make sure we’re only working with data that’s actually exists. While it makes sense to not use missing data, missing data also causes a lot of analysis packages to throw errors! We’ll get rid of anything that’s missing the fields we’re interested in - race percentages, along with number of days wait. We’ll want to compare the number of rows before and after to make sure we don’t lose too many. df.shape ## (12836, 10) df = df.dropna(subset=[&#39;pct_white&#39;, &#39;wait_days&#39;]) df.shape ## (12783, 10) If you end up with a lot of rows that disappear between these two stages, you should take a look to see if something might have gone wrong with your data. In this case we didn’t lose too many, though, so we should be okay. 2.5.3.1 One more cleaning While we’re fixing up our data, I’m going to remove the datetime fields we created. I normally wouldn’t do this, but they’re actually messing with my publishing system! df = df.drop(columns=[&#39;EnterDt&#39;,&#39;PrintDt&#39;,&#39;ResolvDt&#39;,&#39;wait_time&#39;]) "],
["linear-regression.html", "2.6 Linear Regression", " 2.6 Linear Regression 2.6.1 Correlation Before we start getting fancy with a regression, there’s a quick check we can do to see if two variables are related called correlation. Correlation tells you the general idea of “if one number goes up, does another number go up, too?” df[&#39;pct_white&#39;].corr(df[&#39;wait_days&#39;]) x -0.098948 df[&#39;pct_minority&#39;].corr(df[&#39;wait_days&#39;]) x 0.098948 EXPLAIN MORE ABOUT CORRELATION. OR DO WE??? 2.6.2 Linear Regression MORE ABOUT LINEAR REGRESSION Journalistically, linear regression allows us to make statements like “for every X percent increase in minorities in an area, pothole wait times will go up Y days”. Always always always always check your regressions with an expert. You (probably) aren’t a mathematician, and there are a lot of ‘gotchas’ that you can come across when you’re dealing with statistics. In our code, we’ll be asking what the effect of X is on y. No matter what stats package you use, these variable names will generally be the same! In this case, we want to know the affect of pct_minority on wait_days, so X is going to be pct_minority and y is going to be wait_days. In this case, we’re using the statsmodels package for our regression, because it has a real nice-looking output. You run a linear regression with statsmodels like this: import statsmodels.api as sm X = df[[&#39;pct_minority&#39;]] X = sm.add_constant(X) ## /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2495: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead. ## return ptp(axis=axis, out=out, **kwargs) y = df.wait_days model = sm.OLS(y, X) result = model.fit() result.summary() ## &lt;class &#39;statsmodels.iolib.summary.Summary&#39;&gt; ## &quot;&quot;&quot; ## OLS Regression Results ## ============================================================================== ## Dep. Variable: wait_days R-squared: 0.010 ## Model: OLS Adj. R-squared: 0.010 ## Method: Least Squares F-statistic: 126.4 ## Date: Mon, 02 Dec 2019 Prob (F-statistic): 3.49e-29 ## Time: 08:31:39 Log-Likelihood: -50049. ## No. Observations: 12783 AIC: 1.001e+05 ## Df Residuals: 12781 BIC: 1.001e+05 ## Df Model: 1 ## Covariance Type: nonrobust ## ================================================================================ ## coef std err t P&gt;|t| [0.025 0.975] ## -------------------------------------------------------------------------------- ## const 6.0386 0.247 24.489 0.000 5.555 6.522 ## pct_minority 3.9611 0.352 11.242 0.000 3.270 4.652 ## ============================================================================== ## Omnibus: 6385.090 Durbin-Watson: 1.411 ## Prob(Omnibus): 0.000 Jarque-Bera (JB): 37480.668 ## Skew: 2.405 Prob(JB): 0.00 ## Kurtosis: 9.873 Cond. No. 4.68 ## ============================================================================== ## ## Warnings: ## [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. ## &quot;&quot;&quot; The machine that learns from our data is called the model. After the model analyzes each row of our data, it decides the relationship between pct_minority and wait_days, which shows up under the coef section. Under coef it lists pct_minority as 3.9611… but what’s that mean? 2.6.2.1 Understanding the coefficient coef std err t P&gt;|t| [0.025 0.975] -------------------------------------------------------------------------------- const 6.0386 0.247 24.489 0.000 5.555 6.522 pct_minority 3.9611 0.352 11.242 0.000 3.270 4.652 The coefficient - coef - is what goes in our sentence: “for every increase of 1 in pct_minority, pothole wait times will go up Y days”. In this case, the coefficient is 3.9611, so our sentence goes something like this: For every increase of 1 in pct_minority, the number of days you wait is increased by 3.9611 . ( We’d hopefully round it up to around 4, because no one cares about those extra digits.) Now, in this case pct_minority goes from 0-1, with 0 being 0% minorities and 1 being 100% minorities. As a result, “an increase of 1” from our setnence actually means increasing the number of minorities from 0% to 100%. That doesn’t really make sense, so you might do a little division to break it down into smaller units: The output: 1 point increase in pct_minority, an additional 4 days multipled by 10: 0.5 increase in pct_minority, an additional 2 days multipled by 25: 0.25 increase in pct_minority, an additional 1 day As a result: if you have two areas, one with a pct_minority of 0.37 and one with a pct_minority of 0.62 — a 0.25 difference — you can expect pothole fixing to take an extra 1 day in the second area. Very important note: this doesn’t mean a 25% increase in pct_minority (which would be 0.37 + 0.09 = 0.46), it means an actual increase of 0.25 (which would be 0.37 + 0.25 = 0.62). 2.6.2.2 Adjusting our units While can change those numbers around in our heads - 0.25 is 25%, 0.5 is 50%, etc - some people might find that kind of tough to think about. Even though 0-1 can work as a percent, you might have an easier time if we do our analysis with actual percentages. To use “real” percentages, we can just multiply by 100. df[&#39;pct_minority&#39;] = df.pct_minority * 100 df[&#39;pct_white&#39;] = df.pct_white * 100 df.head() address GEOID Geo_FIPS pct_white pct_minority wait_days 0 3839 N 10TH ST 55079004500 55079004500 2.405063 97.59494 1.250000 1 4900 W MELVINA ST 55079003800 55079003800 8.824796 91.17520 8.833333 2 2400 W WISCONSIN AV 55079014900 55079014900 40.313725 59.68627 9.750000 3 1800 W HAMPTON AV 55079002300 55079002300 4.389407 95.61059 2.416667 4 4718 N 19TH ST 55079002300 55079002300 4.389407 95.61059 17.416667 Now that we’ve adjusted our numbers, let’s try out the regression one more time: import statsmodels.api as sm X = df[[&#39;pct_minority&#39;]] X = sm.add_constant(X) y = df.wait_days model = sm.OLS(y, X) result = model.fit() result.summary() ## &lt;class &#39;statsmodels.iolib.summary.Summary&#39;&gt; ## &quot;&quot;&quot; ## OLS Regression Results ## ============================================================================== ## Dep. Variable: wait_days R-squared: 0.010 ## Model: OLS Adj. R-squared: 0.010 ## Method: Least Squares F-statistic: 126.4 ## Date: Mon, 02 Dec 2019 Prob (F-statistic): 3.49e-29 ## Time: 08:31:40 Log-Likelihood: -50049. ## No. Observations: 12783 AIC: 1.001e+05 ## Df Residuals: 12781 BIC: 1.001e+05 ## Df Model: 1 ## Covariance Type: nonrobust ## ================================================================================ ## coef std err t P&gt;|t| [0.025 0.975] ## -------------------------------------------------------------------------------- ## const 6.0386 0.247 24.489 0.000 5.555 6.522 ## pct_minority 0.0396 0.004 11.242 0.000 0.033 0.047 ## ============================================================================== ## Omnibus: 6385.090 Durbin-Watson: 1.411 ## Prob(Omnibus): 0.000 Jarque-Bera (JB): 37480.668 ## Skew: 2.405 Prob(JB): 0.00 ## Kurtosis: 9.873 Cond. No. 161. ## ============================================================================== ## ## Warnings: ## [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. ## &quot;&quot;&quot; The numbers are much smaller, but you might find them easier to deal with. The output: 1 percentage point increase in minorities, an additional .04 days multipled by 10: 10 percentage point increase in minorities, an additional 0.4 days multipled by 25: 25 percentage point increase in minorities, an additional 1 day You’re welcome to multiply, divide, or anything else to your regression units before you do the actual regerssion. Just think about what your final sentence might be and aim for those. Again, if you have two areas, one with a pct_minority of 37% and one with a pct_minority of 62%, you can expect pothole fixing to take an extra 1 day in the second area. And yes, this is not a 25% increase of 37% to 46%, this is an increase of 25 percenage points, 37% + 25% = 62%. 2.6.2.3 Meaning of const Under coef there’s another coefficient we’ve been ignoring named const. coef std err t P&gt;|t| [0.025 0.975] -------------------------------------------------------------------------------- const 6.0386 0.247 24.489 0.000 5.555 6.522 pct_minority 0.0396 0.004 11.242 0.000 0.033 0.047 The basic idea is that linear regression loves the number zero. By default, linear regression on statsmodels assumes that if you have a pct_minority of zero, wait_days will also be zero. Since that’s not true at all, you always need to add in this constant. What’s what the weird .add_constant thing was when we were building our model: X = df[[&#39;pct_minority&#39;]] X = sm.add_constant(X) It means “Hey, model! Zero pct_minority doesn’t mean zero wait_days. Thanks!” And as a result, the model comes up with a const of 6.0386 - the number of days you’ll wait if pct_minority is zero. "],
["alternative-techniques.html", "2.7 Alternative techniques", " 2.7 Alternative techniques We have a problem, though: “for every 25 percentage point increase in minorities, it’s additional 1 day of wait time” just isn’t very understandable. It doesn’t roll off the tongue, it doesn’t make sense very easily, and it’s going to be lost on a lot of your readers. Even though linear regression is a nice advanced-ish method, it doesn’t mean it’s always the right one. Let’s try something easier: df[&#39;majority_white&#39;] = (df.pct_minority &lt; 50).astype(int) df.groupby(&#39;majority_white&#39;).wait_days.median() ## majority_white ## 0 4.208333 ## 1 2.750000 ## Name: wait_days, dtype: float64 2.7.1 Binning While it’s easy to understand majority white vs. majority minority, we could even break it down into a few more categories. While it isn’t as easy as splitting into two groups, it’s a little more nuanced while still being understandable. This is called binning. In the example below, we’ll cut them into brackets of 20 percentage points: 0-20% minority 20-40% minority 40-60% minority 60-80% minority and 80-100% minority bins = range(0, 101, 20) df[&#39;bin&#39;] = pd.cut(df.pct_minority, bins) df.head() ## address GEOID Geo_FIPS pct_white pct_minority \\ ## 0 3839 N 10TH ST 5.507900e+10 5.507900e+10 2.405063 97.594937 ## 1 4900 W MELVINA ST 5.507900e+10 5.507900e+10 8.824796 91.175204 ## 2 2400 W WISCONSIN AV 5.507901e+10 5.507901e+10 40.313725 59.686275 ## 3 1800 W HAMPTON AV 5.507900e+10 5.507900e+10 4.389407 95.610593 ## 4 4718 N 19TH ST 5.507900e+10 5.507900e+10 4.389407 95.610593 ## ## wait_days majority_white bin ## 0 1.250000 0 (80, 100] ## 1 8.833333 0 (80, 100] ## 2 9.750000 0 (40, 60] ## 3 2.416667 0 (80, 100] ## 4 17.416667 0 (80, 100] It seems like we’d use range(0, 100, 20), but nope! Always add one more to make sure your range includes the final number. Now w e can group by the bin and see how a slow increase in demographics affects the wait days. df.groupby(&#39;bin&#39;).wait_days.median() ## bin ## (0, 20] 2.208333 ## (20, 40] 2.916667 ## (40, 60] 3.270833 ## (60, 80] 4.291667 ## (80, 100] 4.250000 ## Name: wait_days, dtype: float64 Way more interesting, right? And much easier to communicate to your readers, to boot. "],
["adding-to-our-analysis.html", "2.8 Adding to our analysis", " 2.8 Adding to our analysis TRYING AGAIN WITH INCOME "]
]
