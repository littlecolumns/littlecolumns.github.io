[
["index.html", "1 Introduction", " 1 Introduction "],
["preface.html", "2 Preface", " 2 Preface We can start with the question: “in the United States, is there a relationship between unemployment in an area and how long people live?” 2.0.1 The original story In a short piece here, Nicky Forster of the Associated Press explained the relationship between demographics and life expectancy with sentences like “an increase of 10 percentage points in the unemployment rate in a neighborhood translated to a loss of roughly a year and a half of life expectancy, the AP found.” Welcome to linear regression! "],
["the-data.html", "3 The Data ", " 3 The Data "],
["turning-our-question-into-measureables.html", "3.1 Turning our question into measureables", " 3.1 Turning our question into measureables First, we’ll need to break this question down into something measureable, then figure out where we can get the necessary information. The question in this case is, “how is life expectancy related to unemployment and other characteristics of an area?” "],
["data-life-expectancy.html", "3.2 Data: Life expectancy", " 3.2 Data: Life expectancy When we’re trying to see if two variables are related - in this case, unemployment (et al) and life expectancy, we need to get our data on a granular basis. Typically we’d be looking for census tract level - one of the smaller units that the Census Burea reports on, which generally encompass between 2,500 and 8,000 people. There are over 65,000 tracts across the United States. Until rather recently, we couldn’t get life expectancy data on a census tract level - but thanks to USALEEP this data is now readily available! The USALEEP project produced estimates of life expectancy at birth—the average number of years a person can expect to live—for most of the census tracts in the United States for the period 2010-2015. Let’s take a note of what the data covers: most of the census tracts in the United States for the period 2010-2015. We’ll grab the life expectancy for the entire USA, a relatively small 2.6MB csv file. 3.2.1 The data dictionary We’ll also grab the data dictionary from the same page, just so we can be aware of what weird column names mean (and any limitations of the data, if they mention them). It’s linked up above, and referred to as the Record Layout "],
["data-unemployment.html", "3.3 Data: Unemployment", " 3.3 Data: Unemployment Unemployment seems like an easy thing to find a number for, but there are a lot of gotchas that make this a meaningful part of the project. 3.3.1 Picking a data source (editorial choice) There are a lot of different places you could get unemployment numbers for the United States, at all sorts of different levels. You can find a breakdown on sites like censusreporter.org, but the Census Bureau has a page all about it! The two the Census Bureau calls attention to are the the Current Population Survey (CPS) and the American Community Survey (ACS). CPS is the source of the monthly “jobs report” politicians talk about, while ACS is a larger and more granular study. The important thing here is we’re looking for data at the census tract level. Browsing the site linked above, the Census Bureau drops a few hints that ACS is going to be our source, but about halfway down the page comes out and says: Because of its large sample size, the ACS will have advantages over the CPS in producing estimates in the following circumstances: to characterize small geographic areas for which CPS (or Local Area Unemployment Statistics Program) estimates are not available, and for comparisons among such areas and between such areas and larger ones; The CPS provides a monthly snapshot of national-level employment and unemployment statistics with less than a 3-week turnaround between the end of data collection in any one month and release of the statistics for that month by the BLS. The ACS does not produce monthly estimates because it has been optimized instead to produce accurate estimates for geographic areas as small as census tracts and block groups. Looks like the ACS is our source! We’re going to obtain our ACS data through Social Explorer (if you’re affiliated with a workplace or school, you might have a free “professional” plan). 3.3.2 Picking a survey year Once of the first questions we get from Social Explorer is which ACS survey are we interested in. American Community Survey data is released every year, and is produced covering different chunks of years. For example: ACS 2010 (1-Year Estimates) ACS 2010 (3-Year Estimates) ACS 2010 (5-Year Estimates) ACS 2013 (3-Year Estimates) ACS 2013 (5-Year Estimates) ACS 2015 (1-Year Estimates) ACS 2015 (5-Year Estimates) ACS 2017 (1-Year Estimates) ACS 2017 (5-Year Estimates) Before we pick one, let’s remember what our USALEEP life expectancy dataset covers: most of the census tracts in the United States for the period 2010-2015 In this case, we want the ACS dataset that most closely matches the time range of our life expectancy numbers. Yes, there is newer unemployment data (2017), but we shouldn’t compare the newest unemployment numbers with older life expectancy data. Things might have changed the two measurements! You might also be tempted to grab a single year, right in the middle of our range - 2013, for example. First, ACS 1-year have much smaller sample sizes than 3- or 5-year datasets, so they aren’t nearly as useful. Secondly - specific to this case - only the 5-year datasets include census tract information. This time around, the 2015 5-year dataset is going to be our best bet. With samples pulled from 2011-2015, it matches up very well with our life expectancy. Doesn’t life expectancy lag behind (…and in front of) geographic factors? That’s a good question, let’s remember it as a discussion topics. 3.3.3 Picking an unemployment table Inside of the American Community Survey, there are multiple tables that cover unemployment, some of which are generated by the ACS and some of which are simpler collections created by Social Explorer. If you browse the list of available tables, there are over 20 tables that include the term “employment.” They’re broken down by all sorts of categories like sex, age, education, disability status and more: |—|—| |code|description| |—|—| |B23001| Sex by Age by Employment Status for the Population 16 Years and Over| |C23002A| Sex by Age by Employment Status for the Population 16 Years and Over (White Alone)| |C23002B| Sex by Age by Employment Status for the Population 16 Years and Over (Black or African American Alone)| |B23006| Educational Attainment by Employment Status for the Population 25 to 64 Years| |B23025| Employment Status for the Population 16 Years and Over| | |…and many, many more.| We’ll be using B23025: Employment Status for the Population 16 Years and Over, because it’s the simplest breakdown that can give us a solid picture of the situation in a given census tract. More importantly, it’s what the AP used. To make this decision, it might have helped to inspect the data for each table. If you you can click through to the table details you can see the actual makeup of the table: Total In Labor Force Civilian Labor Force Employed Unemployed Armed Forces Not in Labor Force With this, we’ll be able to easily see how many people in each census tract are in the labor force but not working. 3.3.4 The data dictionary When you download your data, make sure you scroll to the bottom and get the Data Dictionary. The dataset itself is full of weird codes, and the data dictionary will enable us to understand them. "],
["data-demographics.html", "3.4 Data: Demographics", " 3.4 Data: Demographics The Associated Press ended up looking at more variables than just unemployment, which added a few more selections from the ACS. While they actually looked at even more than the list below, their final analysis involved these tables: B23025 for unemployment C17002 for people just above the poverty line B03002 for racial breakdown (including hispanic) B19013 for median income B06009 for educational attainment We’ll start by just looking at unemployment, but later on we’ll add in more variables from these other tables. "],
["combining-our-datasets.html", "3.5 Combining our datasets", " 3.5 Combining our datasets These datasets are pretty simple to deal with. Since Social Explorer downloads have a million and one columns, though, we’ll need to adjust the options in pandas to cope. import pandas as pd pd.set_option(&quot;display.max_columns&quot;, 200) 3.5.1 Reading in life expectancy The life expectancy data - US_A.CSV - is a standard csv file. life_expect = pd.read_csv(&quot;data/US_A.CSV&quot;) life_expect.head() Tract ID STATE2KX CNTY2KX TRACT2KX e(0) se(e(0)) Abridged life table flag 1001020100 1 1 20100 73.1 2.2348 3 1001020200 1 1 20200 76.9 3.3453 3 1001020400 1 1 20400 75.4 1.0216 3 1001020500 1 1 20500 79.4 1.1768 1 1001020600 1 1 20600 73.1 1.5519 3 The column names are weird, though, and it’s only after looking at the data dictionary - Record_Layout_CensusTract_Life_Expectancy.pdf - that we understand e(0) is going to be our life expectancy column. Let’s rename it! life_expect = life_expect.rename(columns={&#39;e(0)&#39;: &#39;life_expectancy&#39;}) life_expect.head() Tract ID STATE2KX CNTY2KX TRACT2KX life_expectancy se(e(0)) Abridged life table flag 1001020100 1 1 20100 73.1 2.2348 3 1001020200 1 1 20200 76.9 3.3453 3 1001020400 1 1 20400 75.4 1.0216 3 1001020500 1 1 20500 79.4 1.1768 1 1001020600 1 1 20600 73.1 1.5519 3 And while we’re at it, might as well drop the columns we don’t need. Since we’re only using the census tract code - Tract ID - and the life expectancy, it’s slightly easier to select the columns we want instead of using .drop on the ones we don’t. life_expect = life_expect[[&#39;Tract ID&#39;, &#39;life_expectancy&#39;]] life_expect.head() Tract ID life_expectancy 1001020100 73.1 1001020200 76.9 1001020400 75.4 1001020500 79.4 1001020600 73.1 3.5.2 Reading in unemployment An irritating thing about Social Explorer data is that it will always give you an error if you read it into panads. This is because it’s a Latin-1 encoded file, which more or less means is text that only likes American letters and doesn’t support emoji. Pandas doesn’t know this unless you tell it, though. By default pandas assumes every file is text that likes all the world’s languages and supports emoji, which is a format called Unicode or UTF-8. To avoid this error we need to tell pandas to use Latin-1 when opening the file. unemployment = pd.read_csv(&quot;data/R12221544_SL140.csv&quot;, encoding=&#39;latin-1&#39;) unemployment.head() Geo_FIPS Geo_GEOID Geo_NAME Geo_QName Geo_STUSAB Geo_SUMLEV Geo_GEOCOMP Geo_FILEID Geo_LOGRECNO Geo_US Geo_REGION Geo_DIVISION Geo_STATECE Geo_STATE Geo_COUNTY Geo_COUSUB Geo_PLACE Geo_PLACESE Geo_TRACT Geo_BLKGRP Geo_CONCIT Geo_AIANHH Geo_AIANHHFP Geo_AIHHTLI Geo_AITSCE Geo_AITS Geo_ANRC Geo_CBSA Geo_CSA Geo_METDIV Geo_MACC Geo_MEMI Geo_NECTA Geo_CNECTA Geo_NECTADIV Geo_UA Geo_UACP Geo_CDCURR Geo_SLDU Geo_SLDL Geo_VTD Geo_ZCTA3 Geo_ZCTA5 Geo_SUBMCD Geo_SDELM Geo_SDSEC Geo_SDUNI Geo_UR Geo_PCI Geo_TAZ Geo_UGA Geo_BTTR Geo_BTBG Geo_PUMA5 Geo_PUMA1 ACS15_5yr_B23025001 ACS15_5yr_B23025002 ACS15_5yr_B23025003 ACS15_5yr_B23025004 ACS15_5yr_B23025005 ACS15_5yr_B23025006 ACS15_5yr_B23025007 ACS15_5yr_B23025001s ACS15_5yr_B23025002s ACS15_5yr_B23025003s ACS15_5yr_B23025004s ACS15_5yr_B23025005s ACS15_5yr_B23025006s ACS15_5yr_B23025007s 1001020100 14000US01001020100 Census Tract 201, Autauga County, Alabama Census Tract 201, Autauga County, Alabama al 140 0 ACSSF 1760 NaN NaN NaN NaN 1 1 NaN NaN NaN 20100 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 1554 997 997 943 54 0 557 92.12121 85.45455 85.45455 83.63636 18.78788 6.666667 67.87879 1001020200 14000US01001020200 Census Tract 202, Autauga County, Alabama Census Tract 202, Autauga County, Alabama al 140 0 ACSSF 1761 NaN NaN NaN NaN 1 1 NaN NaN NaN 20200 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 1731 884 869 753 116 15 847 143.03030 115.15152 114.54545 107.27273 38.18182 14.545454 86.66667 1001020300 14000US01001020300 Census Tract 203, Autauga County, Alabama Census Tract 203, Autauga County, Alabama al 140 0 ACSSF 1762 NaN NaN NaN NaN 1 1 NaN NaN NaN 20300 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2462 1472 1464 1373 91 8 990 169.09091 132.12121 134.54545 123.03030 31.51515 8.484849 120.60606 1001020400 14000US01001020400 Census Tract 204, Autauga County, Alabama Census Tract 204, Autauga County, Alabama al 140 0 ACSSF 1763 NaN NaN NaN NaN 1 1 NaN NaN NaN 20400 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 3424 2013 1998 1782 216 15 1411 197.57576 157.57576 161.81818 132.12121 58.78788 14.545454 127.87879 1001020500 14000US01001020500 Census Tract 205, Autauga County, Alabama Census Tract 205, Autauga County, Alabama al 140 0 ACSSF 1764 NaN NaN NaN NaN 1 1 NaN NaN NaN 20500 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 8198 5461 5258 5037 221 203 2737 321.81818 339.39394 356.96970 369.09091 89.09091 103.030303 273.93939 While it did open, the result is mildly horrifying. How many columns is that? unemployment.shape ## (74001, 69) The first number is rows, the second one is columns - so, either way, a lot. Most of them are weird geographic identifiers - which we aren’t interested in - and then a handful of employment measures. If we look at our data dictionary we can find the codes for the unemployment columns we’re interested in: ACS15_5yr_B23025001 Total population ACS15_5yr_B23025005 Number unemployed We’re also interested in Geo_FIPS, as it’s our census tract identifier which will let us connect to the life expectancy dataset. Let’s select only the columns we’re interested in to clean things up a bit: unemployment = unemployment[[&#39;Geo_FIPS&#39;, &#39;ACS15_5yr_B23025005&#39;, &#39;ACS15_5yr_B23025001&#39;]] unemployment.head() Geo_FIPS ACS15_5yr_B23025005 ACS15_5yr_B23025001 1001020100 54 1554 1001020200 116 1731 1001020300 91 2462 1001020400 216 3424 1001020500 221 8198 Also, these are the raw numbers of unemployed people. We’re instead interested in percent unemployment, which will allow us to be able to compare census tracts with varying numbers of people. We’ll save the percent unemployment into a new column: unemployment[&#39;unemployed_pct&#39;] = unemployment.ACS15_5yr_B23025005 / unemployment.ACS15_5yr_B23025001 * 100 unemployment.head() Geo_FIPS ACS15_5yr_B23025005 ACS15_5yr_B23025001 unemployed_pct 1001020100 54 1554 3.474903 1001020200 116 1731 6.701329 1001020300 91 2462 3.696182 1001020400 216 3424 6.308411 1001020500 221 8198 2.695779 3.5.3 Final joining Now that we have two datasets with matching census tract columns, we can easily join our datasets together. Both have different names for those columns, though: Tract ID for life expectancy Geo_FIPS for unemployment But it isn’t a problem! When we merge, we’ll just need to use left_on= and right_on=. df = life_expect.merge(unemployment, left_on=&#39;Tract ID&#39;, right_on=&#39;Geo_FIPS&#39;) df.head() Tract ID life_expectancy Geo_FIPS ACS15_5yr_B23025005 ACS15_5yr_B23025001 unemployed_pct 0 1001020100 73.1 1001020100 54 1554 3.474903 1 1001020200 76.9 1001020200 116 1731 6.701329 2 1001020400 75.4 1001020400 216 3424 6.308411 3 1001020500 79.4 1001020500 221 8198 2.695779 4 1001020600 73.1 1001020600 190 2855 6.654991 It’s easy to remember left_on= and right_on= if you think about the actual position of the two dataframes. life_expect is on the left, so left_on= is about its column. Then unemployment is on the right, so Geo_FIPS gets attached to right_on=. Now that our data is cleaned and merged we can get started with our regression. "],
["analyzing-our-data.html", "4 Analyzing our data", " 4 Analyzing our data Now that we’ve prepared our dataset, we just need to perform our analysis. When you want to know “how does X affect this output number,” you’re probably looking at linear regression. In this case we’re looking at how our inputs affect life expectancy, so yes, it’s linear regression. Linear regression is more of a standard statistical technique and less of a machine learning concept, but these days data science and ML are absorbing most everything else numeric in the spirit of “let’s seem fancy.” "],
["linear-regression.html", "4.1 Linear Regression", " 4.1 Linear Regression Before we do our regression, we want to make sure we don’t have any missing data. No one likes missing data, but linear regression dislikes it most of all. If we’re missing numbers for unemployment or life expectancy for any of our census tracts, our regression just plain won’t work! # Check how many rows we have to start with df.shape ## (65662, 6) # Drop everything missing, see how many are left df = df.dropna() df.shape ## (65662, 6) Once we’re sure we have our columns and have ditched any missing data, we’re free to run our regression. When you run a regression you have two variables - and X and a y (and yes, they’re usually capitalized like that). Speaking simply, X is the cause and y is the result - in this case, we’re claiming something similar to “unemployment causes a change in life expectancy,” so X is unemployment and y is life expectancy. Let’s start by pulling out our X, our unemployment percentage. import statsmodels.api as sm X = df[[&#39;unemployed_pct&#39;]] X.head() ## unemployed_pct ## 0 3.474903 ## 1 6.701329 ## 2 6.308411 ## 3 2.695779 ## 4 6.654991 Linear regressions always have one value for y, but can have multiple values for X - for example, later we’ll look at how unemployment and income affect life expectancy. That’s why up above we end up with a dataframe, which can hold multiple possible columns. When we pull out y below, you’ll notice is a series instead, just one single column of values. Unlike our inputs, our outcome is only ever one number. y = df.life_expectancy y.head() ## 0 73.1 ## 1 76.9 ## 2 75.4 ## 3 79.4 ## 4 73.1 ## Name: life_expectancy, dtype: float64 Now that we have our X and y, we can run the actual regression. We’ll be using statsmodels, one of the popular Python packages for doing statistical analysis (another being scikit-learn). I’m going to move setting X and y into the same code block, too, just so we can see it all at once. # Create our X and y X = df[[&#39;unemployed_pct&#39;]] y = df.life_expectancy X = sm.add_constant(X) ## /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2495: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead. ## return ptp(axis=axis, out=out, **kwargs) model = sm.OLS(y,X) results = model.fit() results.summary() ## &lt;class &#39;statsmodels.iolib.summary.Summary&#39;&gt; ## &quot;&quot;&quot; ## OLS Regression Results ## ============================================================================== ## Dep. Variable: life_expectancy R-squared: 0.169 ## Model: OLS Adj. R-squared: 0.169 ## Method: Least Squares F-statistic: 1.336e+04 ## Date: Sun, 01 Dec 2019 Prob (F-statistic): 0.00 ## Time: 21:17:49 Log-Likelihood: -1.7810e+05 ## No. Observations: 65662 AIC: 3.562e+05 ## Df Residuals: 65660 BIC: 3.562e+05 ## Df Model: 1 ## Covariance Type: nonrobust ## ================================================================================== ## coef std err t P&gt;|t| [0.025 0.975] ## ---------------------------------------------------------------------------------- ## const 81.1377 0.028 2856.410 0.000 81.082 81.193 ## unemployed_pct -0.5214 0.005 -115.595 0.000 -0.530 -0.513 ## ============================================================================== ## Omnibus: 616.108 Durbin-Watson: 1.117 ## Prob(Omnibus): 0.000 Jarque-Bera (JB): 807.895 ## Skew: -0.146 Prob(JB): 3.70e-176 ## Kurtosis: 3.459 Cond. No. 12.8 ## ============================================================================== ## ## Warnings: ## [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. ## &quot;&quot;&quot; The code that learns from our data is called the model. After the model analyzes each row of our data, it decides the relationship between unemployed_pct and life_expectancy. This relationship then shows up under the coef section. Under coef it lists unemployed_pct as -0.5214… but what’s that mean? "],
["interpreting-coefficients.html", "4.2 Interpreting coefficients", " 4.2 Interpreting coefficients The coefficient - coef - is what goes in our sentence: “for every increase of 1 in unemployed_pct, life expectancy goes up (or down) Y years”. In this case, the coefficient is -0.5214, so our sentence goes something like this: For every increase of 1 percentage point in the unemployment rate, life expectancy decreases about 6 months (0.5 years). It’s a decrease in life expectancy because the coefficient is negative. 4.2.1 Understanding const Under coef there’s another coefficient we’ve been ignoring named const. coef std err t P&gt;|t| [0.025 0.975] ---------------------------------------------------------------------------------- const 81.1377 0.028 2856.410 0.000 81.082 81.193 unemployed_pct -0.5214 0.005 -115.595 0.000 -0.530 -0.513 The basic idea is that linear regression loves the number zero. By default, linear regression on statsmodels assumes that if you have a unemployed_pct of zero, life_expectancy will also be zero. Linear regression doesn’t really think these things through, does it? By adding this constant, you tell the linear regression that if unemployed_pct is zero, it’s totally okay for life expectancy to be something else. And in this case since const is 81.1377, that’s what life expectancy is with a zero unemployment rate. Also, this is the reason for that weird line sm.add_const line in the regression that we didn’t talk about before. What it does is add a column that’s always 1 to our X, which is a sign to the regression that it’s okay to not start at zero. Take a look: X = df[[&#39;unemployed_pct&#39;]] X = sm.add_constant(X) X ## const unemployed_pct ## 0 1.0 3.474903 ## 1 1.0 6.701329 ## 2 1.0 6.308411 ## 3 1.0 2.695779 ## 4 1.0 6.654991 ## ... ... ... ## 65657 1.0 2.599922 ## 65658 1.0 4.372723 ## 65659 1.0 6.232427 ## 65660 1.0 2.521856 ## 65661 1.0 3.797019 ## ## [65662 rows x 2 columns] "],
["improving-our-analysis.html", "5 Improving our analysis", " 5 Improving our analysis While our analysis is technically correct, there’s a real-life problem: unemployment rate isn’t the only thing that might affect life expectancy. There are plenty of other signals, and the AP went ahead and dug out a few possibilities. Unemployment Percent of people just above the poverty line Racial breakdown of a census tract Median income Educational attainment They grabbed the appropriate tables from the Census (see the data section of this chapter) and made an analysis that took in multiple factors in change of life expectancy across different census tracts, not just unemployment. This is called multivariate linear regression. Let’s reproduce their model with our own census data. 5.0.1 Reading in and processing our data Thankfully Social Explorer combined all of those different tables into one csv file for us, which makes reading in our data super convenient. Although the file is from Social Explorer, though, so we again need to make sure we specify Latin-1 encoding when opening the file. census = pd.read_csv(&quot;data/R12221550_SL140.csv&quot;, encoding=&#39;latin-1&#39;) census.head() Geo_FIPS Geo_GEOID Geo_NAME Geo_QName Geo_STUSAB Geo_SUMLEV Geo_GEOCOMP Geo_FILEID Geo_LOGRECNO Geo_US Geo_REGION Geo_DIVISION Geo_STATECE Geo_STATE Geo_COUNTY Geo_COUSUB Geo_PLACE Geo_PLACESE Geo_TRACT Geo_BLKGRP Geo_CONCIT Geo_AIANHH Geo_AIANHHFP Geo_AIHHTLI Geo_AITSCE Geo_AITS Geo_ANRC Geo_CBSA Geo_CSA Geo_METDIV Geo_MACC Geo_MEMI Geo_NECTA Geo_CNECTA Geo_NECTADIV Geo_UA Geo_UACP Geo_CDCURR Geo_SLDU Geo_SLDL Geo_VTD Geo_ZCTA3 Geo_ZCTA5 Geo_SUBMCD Geo_SDELM Geo_SDSEC Geo_SDUNI Geo_UR Geo_PCI Geo_TAZ Geo_UGA Geo_BTTR Geo_BTBG Geo_PUMA5 Geo_PUMA1 ACS15_5yr_B03002001 ACS15_5yr_B03002002 ACS15_5yr_B03002003 ACS15_5yr_B03002004 ACS15_5yr_B03002005 ACS15_5yr_B03002006 ACS15_5yr_B03002007 ACS15_5yr_B03002008 ACS15_5yr_B03002009 ACS15_5yr_B03002010 ACS15_5yr_B03002011 ACS15_5yr_B03002012 ACS15_5yr_B03002013 ACS15_5yr_B03002014 ACS15_5yr_B03002015 ACS15_5yr_B03002016 ACS15_5yr_B03002017 ACS15_5yr_B03002018 ACS15_5yr_B03002019 ACS15_5yr_B03002020 ACS15_5yr_B03002021 ACS15_5yr_B03002001s ACS15_5yr_B03002002s ACS15_5yr_B03002003s ACS15_5yr_B03002004s ACS15_5yr_B03002005s ACS15_5yr_B03002006s ACS15_5yr_B03002007s ACS15_5yr_B03002008s ACS15_5yr_B03002009s ACS15_5yr_B03002010s ACS15_5yr_B03002011s ACS15_5yr_B03002012s ACS15_5yr_B03002013s ACS15_5yr_B03002014s ACS15_5yr_B03002015s ACS15_5yr_B03002016s ACS15_5yr_B03002017s ACS15_5yr_B03002018s ACS15_5yr_B03002019s ACS15_5yr_B03002020s ACS15_5yr_B03002021s ACS15_5yr_B06009001 ACS15_5yr_B06009002 ACS15_5yr_B06009003 ACS15_5yr_B06009004 ACS15_5yr_B06009005 ACS15_5yr_B06009006 ACS15_5yr_B06009007 ACS15_5yr_B06009008 ACS15_5yr_B06009009 ACS15_5yr_B06009010 ACS15_5yr_B06009011 ACS15_5yr_B06009012 ACS15_5yr_B06009013 ACS15_5yr_B06009014 ACS15_5yr_B06009015 ACS15_5yr_B06009016 ACS15_5yr_B06009017 ACS15_5yr_B06009018 ACS15_5yr_B06009019 ACS15_5yr_B06009020 ACS15_5yr_B06009021 ACS15_5yr_B06009022 ACS15_5yr_B06009023 ACS15_5yr_B06009024 ACS15_5yr_B06009025 ACS15_5yr_B06009026 ACS15_5yr_B06009027 ACS15_5yr_B06009028 ACS15_5yr_B06009029 ACS15_5yr_B06009030 ACS15_5yr_B06009001s ACS15_5yr_B06009002s ACS15_5yr_B06009003s ACS15_5yr_B06009004s ACS15_5yr_B06009005s ACS15_5yr_B06009006s ACS15_5yr_B06009007s ACS15_5yr_B06009008s ACS15_5yr_B06009009s ACS15_5yr_B06009010s ACS15_5yr_B06009011s ACS15_5yr_B06009012s ACS15_5yr_B06009013s ACS15_5yr_B06009014s ACS15_5yr_B06009015s ACS15_5yr_B06009016s ACS15_5yr_B06009017s ACS15_5yr_B06009018s ACS15_5yr_B06009019s ACS15_5yr_B06009020s ACS15_5yr_B06009021s ACS15_5yr_B06009022s ACS15_5yr_B06009023s ACS15_5yr_B06009024s ACS15_5yr_B06009025s ACS15_5yr_B06009026s ACS15_5yr_B06009027s ACS15_5yr_B06009028s ACS15_5yr_B06009029s ACS15_5yr_B06009030s ACS15_5yr_C17002001 ACS15_5yr_C17002002 ACS15_5yr_C17002003 ACS15_5yr_C17002004 ACS15_5yr_C17002005 ACS15_5yr_C17002006 ACS15_5yr_C17002007 ACS15_5yr_C17002008 ACS15_5yr_C17002001s ACS15_5yr_C17002002s ACS15_5yr_C17002003s ACS15_5yr_C17002004s ACS15_5yr_C17002005s ACS15_5yr_C17002006s ACS15_5yr_C17002007s ACS15_5yr_C17002008s ACS15_5yr_B19013001 ACS15_5yr_B19013001s ACS15_5yr_B23025001 ACS15_5yr_B23025002 ACS15_5yr_B23025003 ACS15_5yr_B23025004 ACS15_5yr_B23025005 ACS15_5yr_B23025006 ACS15_5yr_B23025007 ACS15_5yr_B23025001s ACS15_5yr_B23025002s ACS15_5yr_B23025003s ACS15_5yr_B23025004s ACS15_5yr_B23025005s ACS15_5yr_B23025006s ACS15_5yr_B23025007s 1001020100 14000US01001020100 Census Tract 201, Autauga County, Alabama Census Tract 201, Autauga County, Alabama al 140 0 ACSSF 1760 NaN NaN NaN NaN 1 1 NaN NaN NaN 20100 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 1948 1931 1703 150 6 12 0 0 60 0 60 17 17 0 0 0 0 0 0 0 0 123.0303 128.4848 138.7879 76.36364 4.848485 9.696970 6.666667 6.666667 26.66667 6.666667 26.66667 12.727273 12.727273 6.666667 6.666667 6.666667 6.666667 6.666667 6.666667 6.666667 6.666667 1243 184 459 258 166 176 711 118 262 156 84 91 468 44 193 76 82 73 31 5 0 26 0 0 33 17 4 0 0 12 81.81818 44.24242 80.60606 40.60606 36.96970 42.42424 70.90909 30.90909 62.42424 36.36364 32.72727 26.66667 67.87879 24.84848 51.51515 17.57576 23.63636 24.24242 18.787879 5.454546 6.666667 17.575758 6.666667 6.666667 18.18182 12.72727 4.242424 6.666667 6.666667 11.515151 1948 26 132 81 101 125 16 1467 123.0303 18.78788 60.60606 40.60606 58.18182 60.00000 10.90909 127.2727 61838 7212.121 1554 997 997 943 54 0 557 92.12121 85.45455 85.45455 83.63636 18.78788 6.666667 67.87879 1001020200 14000US01001020200 Census Tract 202, Autauga County, Alabama Census Tract 202, Autauga County, Alabama al 140 0 ACSSF 1761 NaN NaN NaN NaN 1 1 NaN NaN NaN 20200 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2156 2139 872 1149 0 50 0 0 68 0 68 17 14 0 0 0 0 3 0 0 0 162.4242 162.4242 125.4545 151.51515 6.666667 36.969697 6.666667 6.666667 37.57576 6.666667 37.57576 15.151515 13.939394 6.666667 6.666667 6.666667 6.666667 4.242424 6.666667 6.666667 6.666667 1397 356 496 342 133 70 1102 295 391 275 94 47 243 43 86 58 33 23 9 0 0 9 0 0 43 18 19 0 6 0 101.21212 69.69697 72.12121 47.27273 29.09091 21.21212 95.15152 68.48485 69.09091 46.06061 24.24242 16.96970 33.93939 22.42424 26.06061 20.00000 13.93939 9.69697 9.090909 6.666667 6.666667 9.090909 6.666667 6.666667 28.48485 18.18182 20.000000 6.666667 4.848485 6.666667 1983 185 320 232 58 34 25 1129 155.1515 110.90909 74.54545 88.48485 25.45455 18.18182 16.96970 144.8485 32303 8204.848 1731 884 869 753 116 15 847 143.03030 115.15152 114.54545 107.27273 38.18182 14.545454 86.66667 1001020300 14000US01001020300 Census Tract 203, Autauga County, Alabama Census Tract 203, Autauga County, Alabama al 140 0 ACSSF 1762 NaN NaN NaN NaN 1 1 NaN NaN NaN 20300 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2968 2968 2212 551 15 41 8 0 141 0 141 0 0 0 0 0 0 0 0 0 0 244.8485 244.8485 225.4545 115.15152 13.333333 37.575758 8.484849 6.666667 81.81818 6.666667 81.81818 6.666667 6.666667 6.666667 6.666667 6.666667 6.666667 6.666667 6.666667 6.666667 6.666667 2074 221 747 674 240 192 1330 119 548 414 135 114 683 87 184 252 82 78 31 0 0 8 23 0 30 15 15 0 0 0 154.54545 53.33333 97.57576 101.21212 49.09091 46.06061 123.63636 35.15152 76.36364 92.72727 34.54545 36.36364 112.12121 34.54545 59.39394 64.84848 33.93939 28.48485 20.000000 6.666667 6.666667 8.484849 17.575758 6.666667 26.66667 16.96970 13.939394 6.666667 6.666667 6.666667 2968 164 213 148 207 82 520 1634 244.8485 138.18182 70.30303 60.60606 78.18182 39.39394 189.09091 175.1515 44922 3411.515 2462 1472 1464 1373 91 8 990 169.09091 132.12121 134.54545 123.03030 31.51515 8.484849 120.60606 1001020400 14000US01001020400 Census Tract 204, Autauga County, Alabama Census Tract 204, Autauga County, Alabama al 140 0 ACSSF 1763 NaN NaN NaN NaN 1 1 NaN NaN NaN 20400 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 4423 3959 3662 162 69 0 0 48 18 5 13 464 30 42 0 0 0 372 20 20 0 298.7879 213.9394 207.8788 80.60606 49.090909 6.666667 6.666667 49.696970 10.30303 4.848485 9.69697 264.848485 20.000000 30.303030 6.666667 6.666667 6.666667 276.363636 17.575758 17.575758 6.666667 2899 339 1044 806 453 257 1623 154 605 458 305 101 1107 106 410 301 148 142 36 0 6 30 0 0 133 79 23 17 0 14 156.36364 78.78788 117.57576 97.57576 76.36364 51.51515 129.69697 43.03030 97.57576 72.72727 67.87879 27.27273 107.27273 36.96970 89.09091 56.96970 38.78788 37.57576 17.575758 6.666667 6.060606 16.363636 6.666667 6.666667 58.18182 60.00000 17.575758 12.121212 6.666667 12.727273 4423 18 74 141 182 583 201 3224 298.7879 17.57576 41.81818 53.33333 58.18182 188.48485 140.00000 331.5152 54329 4244.242 3424 2013 1998 1782 216 15 1411 197.57576 157.57576 161.81818 132.12121 58.78788 14.545454 127.87879 1001020500 14000US01001020500 Census Tract 205, Autauga County, Alabama Census Tract 205, Autauga County, Alabama al 140 0 ACSSF 1764 NaN NaN NaN NaN 1 1 NaN NaN NaN 20500 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 10763 10683 7368 2674 0 412 0 0 229 49 180 80 80 0 0 0 0 0 0 0 0 378.1818 373.3333 482.4242 449.69697 10.909091 146.666667 10.909091 10.909091 100.60606 44.848485 90.30303 43.030303 43.030303 10.909091 10.909091 10.909091 10.909091 10.909091 10.909091 10.909091 10.909091 6974 310 1674 1999 1829 1162 3243 127 967 832 865 452 3432 159 573 1129 884 687 127 0 14 38 52 23 172 24 120 0 28 0 265.45455 102.42424 223.63636 263.03030 255.15152 216.96970 290.30303 55.75758 192.72727 198.78788 183.63636 127.27273 283.63636 82.42424 120.60606 210.30303 152.12121 157.57576 58.787879 10.909091 14.545454 38.181818 26.666667 23.636364 93.93939 27.27273 69.696970 10.909091 29.090909 10.909091 10563 251 952 256 1064 289 89 7662 369.6970 94.54545 521.21212 113.33333 385.45455 162.42424 52.12121 641.8182 51965 4203.030 8198 5461 5258 5037 221 203 2737 321.81818 339.39394 356.96970 369.09091 89.09091 103.030303 273.93939 We thought our unemployment dataset had a lot of columns, but this has even more! If we’re curious, we can take a look: census.shape ## (74001, 189) Seeing as how that’s going to be too many to reasonably drop, and we’re doing a lot of percentage caculations for our new dataset (% unemployed, % just above poverty line, % white, etc), we’re just going to build a brand-new dataframe all at once. It might look a little cleaner than the alterantives. By looking at our data dictionary R12221550.txt, we’re able to figure out what each of the codes mean, which ones we’re interested in, and what the proper calculations are to calculate percentages. census_reg = pd.DataFrame({ &#39;Geo_FIPS&#39;: census.Geo_FIPS, # (1.00 to 1.24 + 1.25 to 1.49) / total &#39;ritp_100_149_pct&#39;: (census.ACS15_5yr_C17002004 + census.ACS15_5yr_C17002005) / census.ACS15_5yr_C17002001 * 100, # Black Alone, Non-Hispanic &#39;black_pct&#39;: census.ACS15_5yr_B03002004 / census.ACS15_5yr_B03002001 * 100, # White Alone, Non-Hispanic &#39;white_pct&#39;: census.ACS15_5yr_B03002003 / census.ACS15_5yr_B03002001 * 100, # All Hispanic &#39;hisp_pct&#39;: census.ACS15_5yr_B03002012 / census.ACS15_5yr_B03002001 * 100, # Population 16 Years and Over: in Labor Force: Civilian Labor Force: Unemployed &#39;unemployed_pct&#39;: census.ACS15_5yr_B23025005 / census.ACS15_5yr_B23025001 * 100, # Educational attainment less than high school &#39;ea_less_than_hs_pct&#39;: census.ACS15_5yr_B06009002 / census.ACS15_5yr_B06009001 * 100, # Median income in 10,000s &#39;median_income_10k&#39;: census.ACS15_5yr_B19013001 / 10000 }) census_reg.head() Geo_FIPS ritp_100_149_pct black_pct white_pct hisp_pct unemployed_pct ea_less_than_hs_pct median_income_10k 1001020100 9.342916 7.700205 87.42300 0.8726899 3.474903 14.802896 6.1838 1001020200 14.624307 53.293135 40.44527 0.7884972 6.701329 25.483178 3.2303 1001020300 11.960916 18.564690 74.52830 0.0000000 3.696182 10.655738 4.4922 1001020400 7.302736 3.662672 82.79448 10.4906172 6.308411 11.693687 5.4329 1001020500 12.496450 24.844374 68.45675 0.7432872 2.695779 4.445082 5.1965 TALK MORE ABOUT THE RITP AND FEATURE SELECTION, MAYBE ITS A SUBSECTION The black, white, and hispanic table and calculations are especially interesting from a “selecting your data” perspetive. I recommend reading the data section of the Milwaukee Journal Sentinel’s pothole-filling analysis to learn a bit more about that. Now that we have a nice dataframe, we can merge our data the exact same way we did before. df = life_expect.merge(census_reg, left_on=&#39;Tract ID&#39;, right_on=&#39;Geo_FIPS&#39;) df.head() Tract ID life_expectancy Geo_FIPS ritp_100_149_pct black_pct white_pct hisp_pct unemployed_pct ea_less_than_hs_pct median_income_10k 0 1001020100 73.1 1001020100 9.342916 7.700205 87.42300 0.8726899 3.474903 14.802896 6.1838 1 1001020200 76.9 1001020200 14.624307 53.293135 40.44527 0.7884972 6.701329 25.483178 3.2303 2 1001020400 75.4 1001020400 7.302736 3.662672 82.79448 10.4906172 6.308411 11.693687 5.4329 3 1001020500 79.4 1001020500 12.496450 24.844374 68.45675 0.7432872 2.695779 4.445082 5.1965 4 1001020600 73.1 1001020600 10.854324 11.918982 72.91613 13.0615425 6.654991 17.487267 6.3092 Looks good, and all set for analysis "],
["performing-the-multivariate-regression.html", "5.1 Performing the multivariate regression", " 5.1 Performing the multivariate regression Remember how linear regression hates missing data? Nothing’s changed since last chapter! Before we perform our regression, we’ll need to drop any rows missing data, whether it’s race data, unemployment data, poverty data, or anything else. # Make note of our original dataframe size df.shape ## (65662, 10) # Drop rows with missing data, compare size df = df.dropna() df.shape ## (65656, 10) Now we can perform our regression. This time instead of picking the columns we want to include in our regression, we’re just going to use .drop to remove the columns we don’t want to include in our regression. First we’ll look at it in isolation before we do the regression. X = df.drop(columns=[&#39;Tract ID&#39;, &#39;Geo_FIPS&#39;, &#39;life_expectancy&#39;]) X = sm.add_constant(X) X.head() Make a note that we drop life_expectancy because it’s what we’re predicting. That makes it our y value, not our X. And even though we have all these extra columns this time, we still need to add the constant - linear regression would still want to make life expectancy zero if each of our columns were zero. Now let’s do our multivariate regression. import statsmodels.api as sm X = df.drop(columns=[&#39;Tract ID&#39;, &#39;Geo_FIPS&#39;, &#39;life_expectancy&#39;]) y = df[&#39;life_expectancy&#39;] X = sm.add_constant(X) model = sm.OLS(y,X) results = model.fit() results.summary() ## &lt;class &#39;statsmodels.iolib.summary.Summary&#39;&gt; ## &quot;&quot;&quot; ## OLS Regression Results ## ============================================================================== ## Dep. Variable: life_expectancy R-squared: 0.490 ## Model: OLS Adj. R-squared: 0.490 ## Method: Least Squares F-statistic: 8997. ## Date: Sun, 01 Dec 2019 Prob (F-statistic): 0.00 ## Time: 21:17:54 Log-Likelihood: -1.6208e+05 ## No. Observations: 65656 AIC: 3.242e+05 ## Df Residuals: 65648 BIC: 3.243e+05 ## Df Model: 7 ## Covariance Type: nonrobust ## ======================================================================================= ## coef std err t P&gt;|t| [0.025 0.975] ## --------------------------------------------------------------------------------------- ## const 81.2365 0.122 665.628 0.000 80.997 81.476 ## ritp_100_149_pct -0.0596 0.003 -21.738 0.000 -0.065 -0.054 ## black_pct -0.0666 0.001 -56.960 0.000 -0.069 -0.064 ## white_pct -0.0386 0.001 -36.707 0.000 -0.041 -0.037 ## hisp_pct 0.0131 0.001 10.298 0.000 0.011 0.016 ## unemployed_pct -0.1490 0.004 -33.408 0.000 -0.158 -0.140 ## ea_less_than_hs_pct -0.0862 0.002 -48.979 0.000 -0.090 -0.083 ## median_income_10k 0.4825 0.006 83.217 0.000 0.471 0.494 ## ============================================================================== ## Omnibus: 2114.193 Durbin-Watson: 1.520 ## Prob(Omnibus): 0.000 Jarque-Bera (JB): 4788.035 ## Skew: 0.183 Prob(JB): 0.00 ## Kurtosis: 4.271 Cond. No. 790. ## ============================================================================== ## ## Warnings: ## [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. ## &quot;&quot;&quot; So many numbers! First, notice that the coefficient for unemployed_pct changed, changing from -0.5214 to -0.1490. Now instead of losing half a year of life expectancy for a 1% increase in employment, you now lose about 0.15 years (a little under two months). Why did this happen? When you do a multivariate regression, the regression is considering all of the factors at the same time. So now instead of thinking that unemployment is the only factor going into life expectancy, the regression is also weighing racial breakdown, income, and the other fields. The new regression then discovered that some of the variation that the first regression said was due to unemployment is actually better explained by those other fields, so the coefficient for unemployment moved closer to zero. Let’s take a look at all of the coefficients and see what we get. |—|—|—| |variable|coefficient|meaning| |—|—|—| |const|81.2365|If everything else is 0, life expectancy will be about 81 years| |ritp_100_149_pct|-0.0596|For every 1 percentage point increase in people just above the poverty line, life expectancy goes down 0.06 years (~3 weeks)| |black_pct|−0.0666|For every 1 percentage point increase in the black population, life expectancy decreases by 0.07 years (~3 and a half weeks)| |white_pct|−0.0386|For every 1 percentage point increase in the white population, life expectancy goes down by 0.04 years (~2 weeks)| |hisp_pct|0.0131|For every 1 percentage point increase in the hispanic population, life expectancy goes up 0.01 years (~4 days)| |unemployed_pct|−0.1490|For every 1 percentage point increase in the unemployed population, life expectancy goes down 0.15 years (~8 weeks)| |ea_less_than_hs_pct|−0.0862|For every 1 percentage point increase in people with less than high school education, life expectancy goes down 0.09 years (~4.5 weeks)| |median_income_10k|0.4825|For every additional $10,000 in median income in an area, life expectancy increases 0.49 years (~6 months)| So now we can find that line from the final published piece: An increase of 10 percentage points in the unemployment rate in a neighborhood translated to a loss of roughly a year and a half of life expectancy, the AP found. A neighborhood where more adults failed to graduate high school had shorter predicted longevity. While the regression was dealing with one percentage point of unemployment, saying “one percentage point loses roughly 8 weeks of life expectancy” is not nearly as impactful. If we multiply both sides by ten, we get something much more publishable: 1 percentage point becomes 10 percentage points, and −0.15 years becomes −1.5 years. And then we’re published in the AP! "],
["conclusion.html", "6 Conclusion", " 6 Conclusion While the regression was straightforward in terms of code and there wasn’t too much cleaning to do, there are still alternative ways of performing regressions and plenty of data-related questions we might ask. To follow up with these, be sure to check the other notebooks and the discussion topics on the project page. "]
]
