[
["index.html", "1 Introduction", " 1 Introduction "],
["preface.html", "2 Preface ", " 2 Preface "],
["the-original-story.html", "2.1 The original story", " 2.1 The original story When people write FOIA requests, it can be pretty stressful to wonder if they’re going to get fulfilled. Unless you’ve done it many times before, the process might feel like a little bit of a black box. what exactly do you need to say in order your request filled? Enter the FOIA Predictor! You insert your FOIA request, you say who you’re sending it to, and it gives you an estimate of what your chances look like. The FOIA Predictor is an excellent case study in a lot of things that do with machine learning: classification problems feature selection algorithm choice evaluation metrics explainability community development and a whole lot more! FOIA Predictor was written by Rachel Downs when she was an intern at data.world, which we’re going to use to explain away any of the rough edges the FOIA Predictor might have. You can’t hold anything an intern does against them, because: they’re an intern! "],
["what-it-does.html", "2.2 What it does", " 2.2 What it does The way the FOIA predictor works is that it takes a big long list of FOIA requests - successful and not successful - and calculates a bunch of measurements for each. How many sentences are in it, how long an average sentence is, if you reference FOIA fees, and so on. It then looks at each of those metrics - “feature,” we call them - to see how they’re related to whether a request is successful or not. Then you take your own request and feed it to the predictor. The predictor calculates all the same metrics as it did on the other requests - sentence length, etc - and uses them to predict whether your request will be successful or not. When the FOIA Predictor is used on requests it hasn’t seen before (but requests that we know whether they were successful or not), it’s right about 80% of the time, which sounds amazing. It’s that 80% accuracy rate that we’re interested in now. First, let’s see how that accuracy rate is calculated. We’re going to be following along with the FOIA Predictor’s code, because wonderfully enough, it’s all open source. We’ll talk more about that later. "],
["our-data.html", "3 Our data ", " 3 Our data "],
["data-foia-requests.html", "3.1 Data: FOIA Requests", " 3.1 Data: FOIA Requests The FOIA Predictor’s data comes from MuckRock, a “a non-profit, collaborative news site that brings together journalists, researchers, activists, and regular citizens to request, analyze, and share government documents, making politics more transparent and democracies more informed.” MuckRock maintains a system that you can use to file and track FOIA requests, which makes it an easy source of data on both successful and unsuccessful requests. We’re going to cheat a little bit and use the same dataset that FOIA Predictor uses instead of downloading and creating our own. "],
["success-metrics-editorial-choice.html", "3.2 Success metrics (editorial choice)", " 3.2 Success metrics (editorial choice) .table-wrapper { font-size: 12px; overflow-x: scroll; max-height: 75vh; margin-bottom: .85em } .table-wrapper td { padding: 0.5em 0.5em } .table-wrapper th { position: sticky; top: -1px; background: white; vertical-align: bottom; } .table-wrapper td { vertical-align: top } Let’s read this data in! Since we’re using the same dataset that FOIA Predictor uses, it with a lot of extras. Along with the request itself, it also calculated bits and pieces, too, like average sentence length and a readability score. import pandas as pd pd.set_option(&quot;display.max_columns&quot;, 20) pd.set_option(&quot;display.max_colwidth&quot;, 100) columns = [&#39;trackingID&#39;, &#39;title&#39;, &#39;agency&#39;, &#39;date_submitted&#39;, &#39;closed_date&#39;, &#39;url&#39;, &#39;status&#39;, &#39;char_count&#39;, &#39;word_count&#39;, &#39;ref_data&#39;, &#39;sen_count&#39;, &#39;avg_sen_len&#39;, &#39;closed_datetime&#39;, &#39;ref_foia&#39;, &#39;ref_fees&#39;, &#39;phone_number&#39;, &#39;hyperlink&#39;, &#39;email_address&#39;, &#39;ref_date&#39;, &#39;readability&#39;, &#39;specificity&#39;, &#39;high_success_rate_agency&#39;, &#39;request&#39;] df = pd.read_csv(&quot;data/recent-requests-data-for-model.csv&quot;, usecols=columns) df.head(3) trackingID title agency date_submitted closed_date url status request char_count word_count sen_count avg_sen_len closed_datetime ref_foia ref_fees phone_number hyperlink email_address ref_date readability ref_data specificity high_success_rate_agency 24540 “02/29/16 - SLCPD Abdi Mohamed Protest Action Plan and Debrief Docs” 4223 2016-03-18 2016-04-11 /foi/salt-lake-city-359/022916-slcpd-abdi-mohamed-protest-action-plan-and-debrief-docs-24540/ no_docs “- Action Plan(s) for policing demonstrations, protests and other events on February 29, 2016]. Please include all draft and final versions along with any associated metadata. - After Action Reports and/or written debriefs of demonstrations, protests and other events on the day. Please include all draft and final versions along with any associated metadata.. Please include in your search, any and all documents, correspondence within and outside the department, emails, training presentations, PowerPoint slides, and Microsoft Word documents, which discuss the demonstrations and or protests through the date of this request. The requested documents will be made available to the general public free of charge as part of the public information service at MuckRock.com, and is not being made for commercial usage. In the event that fees cannot be waived, I would be grateful if you would inform me of the total charges in advance of fulfilling my request. I would prefer the request filled electronically, by e-mail attachment if available or CD-ROM if not. Thank you in advance for your anticipated cooperation in this matter. I look forward to receiving your response to this request within 5 business days, as the statute requires.” 1008 194 9 21.55556 2016-04-11 0 1 0 0 0 1 13.820355 1 8 0 34051 “1026812 documents” 503 2017-02-26 2017-03-20 /foi/chicago-169/1026812-documents-34051/ done “The following documents from the IAD investigation under the CR number 1026812, identified by their attachment number and description: 6. Synoptic report of Sgt. Richard Downs 19. Handwritten statement of Lt. John Brundage 24-30. Interviews with Cmdr. Leo Schmitz, Lt. John Brundage, Sgt. Patrick Quinn, P.O. Brenda Gomez-Sanchez, P.P.O Milton Kinnison, and Sgt. Sean Ronan 54-55. Results of email account searches of Cmdr. Leo Schmitz and Sgt. Sean Ronan 56-59. All in-car camera footage 78. Cmdr. Leo Schmitz’s Blackberry log 80. Cmdr. Leo Schmitz’s response to OCIC report 81. OCIC report retrieved from Sgt. Sean Ronan’s email 87. Cmdr. Leo Schmitz’s disciplinary history 89. Lt. John Brundage’s disciplinary history” 568 114 12 9.50000 2017-03-20 0 0 1 0 0 0 6.787368 0 32 1 31682 “1033 MOU and annual (2015/16) inventory form (Illinois Dept of CMS)” 4074 2017-01-05 2017-01-20 /foi/illinois-168/1033-mou-and-annual-201516-inventory-form-illinois-dept-of-cms-31682/ done \"-The current memorandum of agreement (MOA) or memorandum of understanding (MOU) with the Defense Logistics Agency, Disposition Services regarding the 1033 equipment surplus program administered by the DLA Law Enforcement Support Office -The annual inventory form for years 2015 and 2016 required to be completed by the state coordinator of the 1033 program According to the DLA FAQ page regarding the 1033 program, CMS is the agency in charge of coordinating the 1033 program for Illinois. See http://www.dispositionservices.dla.mil/leso/Pages/StateCoordinatorList.aspx\"; 473 79 2 39.50000 2017-01-20 0 0 0 1 0 1 20.000000 0 9 1 When you read in the dataset of fulfilled FOIA requests, you immediately need to make some editorial decisions. The thing we’re looking for - whether a request was fulfilled or denied - is not actually exactly in the dataset. What our dataset has instead is a status column. The statuses look like this: df.status.value_counts() ## done 2749 ## no_docs 1879 ## processed 1491 ## ack 945 ## rejected 739 ## fix 455 ## abandoned 304 ## payment 247 ## appealing 176 ## partial 82 ## submitted 27 ## Name: status, dtype: int64 We see denied in there and done technically means fulfilled, but we also see a lot of other things. We see when the request is the agency says the documents don’t exist, or they aren’t responding to emails, or the requester isn’t responding to emails, or more things that might not be totally clear. The question is what counts as fulfilled, and do we use all of these documents? Let’s look at some of our options. Option One: Remove everything except documents that were marked as either accepted or rejected. Accepted counts as fulfilled, denied counts as not funfilled. Option Two: Keep everything. Accepted counts as fulfilled, everything not ‘accepted’ counts as denied. **Option Three: Accepted counts as fulfilled, denied counts as not fulfilled. “Documents don’t exist” also counts as denied, because maybe you were just too vague, or too specific. “Abandoned” counts as denied, because again, maybe you were too vague or too specific in your original request. Options Four through One Hundred: You have a lot of options here. Picking which ones matter, picking what counts as fulfilled, what counts as denied. And there might not a right answer, maybe you and someone else have different ideas about what counts as a fulfilled request. The FOIA Predictor uses Option Two, which casts the most narrow net for accepted and the widest net for denied. Because machine learning loves to do things with numbers, we’re now going to count 1 as success, and 0 as a denied. df[&#39;successful&#39;] = (df.status == &quot;done&quot;).astype(int) df.successful.value_counts() ## 0 6345 ## 1 2749 ## Name: successful, dtype: int64 More denials than successful requests, but I’m actually impressed at how many successes we have! "],
["features-editorial-choice.html", "3.3 Features (editorial choice)", " 3.3 Features (editorial choice) Usually at this point we would think about what measurements to take for each FOIA request. These features are what we use to describe a request. In this case we don’t have to think very hard: the work was already done for us! The FOIA Predictor csv file came with a lot of pre-computed features, and we’re just going to use the ones that FOIA Predictor uses: word count average sentence length references to FOIA law references to fees inclusion of hyperlinks inclusion of email address specificity whether the agency fulfills &gt;50% of requested FOIA requests There are more features available in our dataset - readability, for example - but FOIA Predictor only uses the ones above, so we’ll copy them. features = df[[ &#39;successful&#39;, &#39;high_success_rate_agency&#39;, &#39;word_count&#39;, &#39;avg_sen_len&#39;, &#39;ref_foia&#39;, &#39;ref_fees&#39;, &#39;hyperlink&#39;, &#39;email_address&#39;, &#39;specificity&#39; ]] features.head() successful high_success_rate_agency word_count avg_sen_len ref_foia ref_fees hyperlink email_address specificity 0 0 194 21.55556 0 1 0 0 8 1 1 114 9.50000 0 0 0 0 32 1 1 79 39.50000 0 0 1 0 9 1 0 44 14.66667 0 0 1 0 2 0 1 24 24.00000 0 0 0 0 3 It’s easy enough to just follow in their tracks, right? The high_success_rate_agency field is going to be an interesting one, but let’s move on to our analysis for now. "],
["analysis.html", "4 Analysis ", " 4 Analysis "],
["designing-our-model.html", "4.1 Designing our model", " 4.1 Designing our model Now that our data is prepared, we need to build the thing that’s going to make the predictions for us, the thing we ask: “excuse me, but is this FOIA request going to succeed?” In technical terms this “thing” is called a model, because it models the interaction between the sentence length, etc, and whether our request is accepted or denied. 4.1.1 Classification problems Any time you’re trying to predict a yes/no or a category, you’re looking at a classification problem. With a classification problem you have examples of both categories, and you train the model about the difference between the two groups. In this case, we have two classes of documents: successful or unsuccessful, 1 and 0. We also have a set of documents that we know are in one catgory or the other. We’ll use these known documents to then make predictions about unknown documents (the FOIA you submit to the Predictor). "],
["selecting-an-algorithm.html", "4.2 Selecting an algorithm", " 4.2 Selecting an algorithm When you’re doing classification, there isn’t just one way to do it! You have a whole menu of algorithms you could use, each of which have different technical and statistical approaches. Although they’re different, they’re all trying to do the same thing: predict whether your request will be granted or not. For now we’ll stick with the one that FOIA Predictor uses: k-nearest neighbors. While the specifics of how it works don’t really matter, if you want to know how it works, you can read more here. from sklearn.neighbors import KNeighborsClassifier knn = KNeighborsClassifier(n_neighbors=20) "],
["training-our-algorithm.html", "4.3 Training our algorithm", " 4.3 Training our algorithm Now that we’ve decided on our algorithm, we can take our classifier and teach it about our documents. This is called training. There’s one problem, though! Just because we tried to make an algorithm doesn’t mean it’s gonna be any good. And it sure would be rude to put a classifier out into the world that doesn’t quite work, right? Before we start using our classifier in the world, we need some way of judging its performance. This is called testing. Testing in machine learning works a lot like testing in a normal classroom. You have a big test coming up. Your teacher gives you some sample problems or maybe previous tests to study, all of which have the correct answers included. On test day, she gives you new problems, ones you’ve never seen before. They’re similar to the sample problems you studied, though. Unlike you, teacher knows the answers, so when you hand in your test she can judge how well you peformed. Note that the study problems and the test problems aren’t the same problems. If they were, you could just memorize the answers! Instead, your instructor gave you enough sample problems to study that you have a pretty good idea of what the test problems will be like. If we want to run our machine learning world like a class, the first thing we need to do is split up our documents into ones we’ll use for training and ones we’ll use for testing. from sklearn.model_selection import train_test_split X = features.drop(&#39;successful&#39;, axis=1) y = features.successful X_train, X_test, y_train, y_test = train_test_split(X, y) After we split them into these two groups - training and testing - we’ll give the training dataset to the algorithm for studying. This training is when it (hopefully) learns the difference between a successful and unsuccessful FOIA request. knn.fit(X_train, y_train) ## KNeighborsClassifier(algorithm=&#39;auto&#39;, leaf_size=30, metric=&#39;minkowski&#39;, ## metric_params=None, n_jobs=None, n_neighbors=20, p=2, ## weights=&#39;uniform&#39;) Once the classifier is done training, we can test it! We’ll feed it the test data, and see if it thinks each FOIA request succeeded or failed. y_pred = knn.predict(X_test) In an ideal world these predictions will match whether the requests were actually successful or not. Unlike a classroom test, though, you’ll rarely see a machine learning algorithm do a perfect job. "],
["evaluation-metrics.html", "4.4 Evaluation metrics", " 4.4 Evaluation metrics When you’ve put together a machine learning algorithm, you need some way to test its performance. This is called an evaluation metric. It seems like it would be as easy as your teacher scoring a test, but it gets more complicated pretty quickly. 4.4.1 Accuracy The most basic way we can judge its performance is to ask: what percent did you predict correctly? We’ll check by comparing the right answers to what the classifier predicted. from sklearn.metrics import accuracy_score accuracy_score(y_test, y_pred) ## 0.7484608619173263 Around 75% percent, not so bad! Unfortunately, there’s a little issue with accuracy that makes it almost useless as a metric. Important question: How often is a request denied, and how often is it accepted? df.successful.value_counts() ## 0 6345 ## 1 2749 ## Name: successful, dtype: int64 6,345 denied requests, 2,749 requests fulfilled. If we take one more step convert that to percentages, our crisis might be a little more clear. df.successful.value_counts(normalize=True) ## 0 0.697713 ## 1 0.302287 ## Name: successful, dtype: float64 Yes, we have around 70% denied. So what? We didn’t think it was a bad split before - 30% success rate, not so tough. Here’s the problem: if our classifier just guessed “it’s gonna get rejected” every single time, we’d be 70% accurate! Even though we’d be throwing out every single successful request, it wouldn’t matter. If we’re accuracy as our evaluation metric, it doesn’t matter what we got right and what we got wrong, it only counts the number of correct predictions. 4.4.2 Dummy classifier You can also do this with code, too, using a hilarious classifier called a DummyClassifier. It isn’t a real classifier, but you can tell it to just guess the most popular thing! The code works just like a ‘normal’ classifier, which I find incredibly amusing. from sklearn.dummy import DummyClassifier dummy = DummyClassifier(strategy=&#39;most_frequent&#39;, random_state=42) dummy.fit(X_train, y_train) ## DummyClassifier(constant=None, random_state=42, strategy=&#39;most_frequent&#39;) So let’s say we use the dummy classifier. How does it do just guessing the most popular thing, no machine learning in sight? accuracy_score(y_test, dummy.predict(X_test)) ## 0.7036059806508356 Just like we predicted, right around 70%. Not feeling so good about our 75%ish performance now, are we? 4.4.3 Confusion matrix It’s turned out that what we’re interested in isn’t just “did you get it right?” What we’re interested in is somehow looking at acceptances and denials and making sure we didn’t just throw everything into one bucket or the other. To look at how we peformed on different aspects of our classification ask, we can use a confusion matrix. Let’s see how our k-nearest neighbors classifier performed. from sklearn.metrics import confusion_matrix y_true = y_test y_pred = knn.predict(X_test) matrix = confusion_matrix(y_true, y_pred) label_names = pd.Series([&#39;unsuccessful&#39;, &#39;successful&#39;]) pd.DataFrame(matrix, columns=&#39;Predicted &#39; + label_names, index=&#39;Is &#39; + label_names) Predicted unsuccessful Predicted successful Is unsuccessful 1506 94 Is successful 478 196 A confusion matrix can put into context where we’re making our mistakes. And here’s the confusion matrix for the dummy classifier. y_true = y_test y_pred = dummy.predict(X_test) matrix = confusion_matrix(y_true, y_pred) label_names = pd.Series([&#39;unsuccessful&#39;, &#39;successful&#39;]) pd.DataFrame(matrix, columns=&#39;Predicted &#39; + label_names, index=&#39;Is &#39; + label_names) Predicted unsuccessful Predicted successful Is unsuccessful 1600 0 Is successful 674 0 Unlike the accuracy score, we have a clear distinction between the two! It’s easy to see that the dummy classifier doesn’t predict anything as successful, while the k-nearest neighbors classifier is a bit more mixed. A confusion matrix is a great way to see how your classifier performs across your classes separately. Successful, unsuccessful, all broken out. It’s really really easy to see how different they are when viewed this way. Each one of those boxes has a name. term meaning where True Positive is successful, predicted successful top left False Negative is successful, predicted unsuccessful top right False Positive is unsuccessful, predicted successful bottom left True Negative is unsuccessful, predicted unsuccessful bottom right If we’re being frank, though: I really dislike using those names. I can barely remember which is which, and I think using sterile names like that makes you forget what you’re actually working on. For example, “minimize false negatives.” In normal words, you might say someting like “be really sure someone’s reject is going to be rejected before you tell them that, because they might not submit it.” I think keeping the language related to the topic helps us understand what we’re really doing, and what impact we’re really having. But either way, our K-Nearest Neighbors seems to be working pretty well. Or at least, it goes a bit better than always guessing ‘denied.’ 4.4.4 Explainability Hand-in-hand with performance is the idea of explainability. Why did our algorithm give the result it did? Imagine how uncomfortable it would be dealing with a person who could never explain their reasoning behind their decisions! One problem of the K-Nearest Neighbors algorithm is that it’s kind of difficult to explain what’s going on, or why we received a certain result. In theory it’s easy - “we’re taking the 20 most similar FOIA requests and picking whether they were fufilled or not” - but it’s difficult to point out exactly which columns are the important ones, or give feedback on how we might need to improve your FOIA request. Maybe it’s time to look at some alternatives? As we mentioned before, there are more classification algorithms than just K-Nearest Neighbors. A few examples are logistic regression classifiers, decision trees, and random forests. While there is a lot of potential math and tests for suitability between your dataset and what kind of algorithm should work best, at the end of the day the only thing that really matters is which one does work best. To figure it out… you just try all of the different algorithms and compare the results! "],
["other-classifiers.html", "5 Other classifiers ", " 5 Other classifiers "],
["logisic-regression.html", "5.1 Logisic Regression", " 5.1 Logisic Regression The logistic regression classifier is one alternative to the k-nearest neighbors classifier. from sklearn.linear_model import LogisticRegression logreg = LogisticRegression(C=1e9, solver=&#39;lbfgs&#39;, max_iter=1000) logreg.fit(X_train, y_train) # Check its accuracy ## LogisticRegression(C=1000000000.0, class_weight=None, dual=False, ## fit_intercept=True, intercept_scaling=1, l1_ratio=None, ## max_iter=1000, multi_class=&#39;warn&#39;, n_jobs=None, penalty=&#39;l2&#39;, ## random_state=None, solver=&#39;lbfgs&#39;, tol=0.0001, verbose=0, ## warm_start=False) accuracy_score(y_test, logreg.predict(X_test)) ## 0.7897977132805629 Accuracy looks like it’s a few points better, but we know to not trust that as an evaluation metric. y_true = y_test y_pred = logreg.predict(X_test) matrix = confusion_matrix(y_true, y_pred) label_names = pd.Series([&#39;unsuccessful&#39;, &#39;successful&#39;]) pd.DataFrame(matrix, columns=&#39;Predicted &#39; + label_names, index=&#39;Is &#39; + label_names) Predicted unsuccessful Predicted successful Is unsuccessful 1400 200 Is successful 278 396 It turns out a logistic regression does way better than k-nearest neighbors! For our successful requests, KNN predicted just shy of 200 of them. Logistic regression was able to predict over four hundred! The KNN did better with our unsuccessful results, though - it predicted around ~1500 compared to the logistic regression classifier’s ~1400. When facing a choice between different classifiers, these are the sorts of things that might make you go in one direction or another. If we do better at identifying successful requests, is it okay to have a few unsuccessful requests incorrectly predicted as successful? Along with performing better than a KNN algorithm, logistic regression is also remarkably convenient when it comes to explaining the result! We can use the Python package eli5 to see which columns are important. import eli5 eli5.explain_weights_df(logreg, feature_names=list(X.columns)) target feature weight 1 high_success_rate_agency 2.3467777 1 ref_foia 0.4803175 1 email_address 0.0847820 1 word_count 0.0006296 1 avg_sen_len -0.0022111 1 specificity -0.0330302 1 hyperlink -0.1136048 1 ref_fees -0.4810744 1 &lt;BIAS&gt; -1.3265562 From these results we can see that by far the most important thing is whether you’re applying to a high-success-rate agency. We’ll address that later, let’s move on to another classifier. There’s a lot more to be said about logistic regression! After you’re done here, each one of these classifiers has more examples and explanations in other chapters. "],
["decision-trees.html", "5.2 Decision Trees", " 5.2 Decision Trees A decision tree is another classifier we can try out. Can it beat logistic regression? from sklearn import tree dec_tree = tree.DecisionTreeClassifier(max_depth=4) dec_tree.fit(X_train, y_train) # Check its accuracy ## DecisionTreeClassifier(class_weight=None, criterion=&#39;gini&#39;, max_depth=4, ## max_features=None, max_leaf_nodes=None, ## min_impurity_decrease=0.0, min_impurity_split=None, ## min_samples_leaf=1, min_samples_split=2, ## min_weight_fraction_leaf=0.0, presort=False, ## random_state=None, splitter=&#39;best&#39;) accuracy_score(y_test, dec_tree.predict(X_test)) ## 0.7990325417766051 y_true = y_test y_pred = dec_tree.predict(X_test) matrix = confusion_matrix(y_true, y_pred) label_names = pd.Series([&#39;unsuccessful&#39;, &#39;successful&#39;]) pd.DataFrame(matrix, columns=&#39;Predicted &#39; + label_names, index=&#39;Is &#39; + label_names) Predicted unsuccessful Predicted successful Is unsuccessful 1433 167 Is successful 290 384 The decision tree performs similarly to the logistic regression, but it has one big big benefit: to explain it, we can draw a super-fun diagram. import pydotplus # from IPython.display import Image dot_data = tree.export_graphviz(dec_tree, max_depth=3, feature_names=X.columns, class_names=dec_tree.classes_.astype(str), out_file=None, filled=True, rounded=True, proportion=True) graph = pydotplus.graph_from_dot_data(dot_data) # Image(graph.write_png(&quot;output.png&quot;)) graph.write_png(&quot;output.png&quot;) I can also get a similar chart to the logistic regression one, explaining why a particular feature may or may not be important. eli5.explain_weights_df(dec_tree, feature_names=list(X.columns)) feature weight high_success_rate_agency 0.8813444 specificity 0.0592870 word_count 0.0350351 avg_sen_len 0.0117031 ref_foia 0.0108913 ref_fees 0.0017391 email_address 0.0000000 hyperlink 0.0000000 Again, it’s the high_success_rate_agency feature that does heavy lifting. "],
["random-forest.html", "5.3 Random Forest", " 5.3 Random Forest A random forest is a collection of slightly different decision trees, hence the name. If you compare the output, you can get a performance boost over a single decision tree. from sklearn.ensemble import RandomForestClassifier forest = RandomForestClassifier(n_estimators=100) forest.fit(X_train, y_train) ## RandomForestClassifier(bootstrap=True, class_weight=None, criterion=&#39;gini&#39;, ## max_depth=None, max_features=&#39;auto&#39;, max_leaf_nodes=None, ## min_impurity_decrease=0.0, min_impurity_split=None, ## min_samples_leaf=1, min_samples_split=2, ## min_weight_fraction_leaf=0.0, n_estimators=100, ## n_jobs=None, oob_score=False, random_state=None, ## verbose=0, warm_start=False) Even though we don’t enjoy accuracy, we might as well check it. # Check its accuracy accuracy_score(y_test, forest.predict(X_test)) ## 0.7963940193491644 And now the confusion matrix. y_true = y_test y_pred = forest.predict(X_test) matrix = confusion_matrix(y_true, y_pred) label_names = pd.Series([&#39;unsuccessful&#39;, &#39;successful&#39;]) pd.DataFrame(matrix, columns=&#39;Predicted &#39; + label_names, index=&#39;Is &#39; + label_names) Predicted unsuccessful Predicted successful Is unsuccessful 1438 162 Is successful 301 373 eli5.explain_weights_df(forest, feature_names=list(X.columns)) feature weight std high_success_rate_agency 0.3416595 0.0218430 word_count 0.2330515 0.0207982 avg_sen_len 0.2272921 0.0149889 specificity 0.1571883 0.0239631 hyperlink 0.0129037 0.0059737 ref_foia 0.0117621 0.0052155 ref_fees 0.0096072 0.0036738 email_address 0.0065355 0.0034851 It does a bit better than the other classifiers we’ve seen so far, and we can see it’s using more than just the ever-popular high_success_rate_agency column to figure out whether it will be granted or not. "],
["feature-selection-and-engineering.html", "6 Feature selection and engineering", " 6 Feature selection and engineering One thing that always bothered me about the FOIA Predictor is that high_success_rate_agency column. It seems like all of the classifiers really love it, what if we remove every other feature except for that one? Let’s see what it looks like with a logistic regression using only high_success_rate_agency. # Only select one feature this time X = df[[&#39;high_success_rate_agency&#39;]] y = df.successful # Train/test split X_train, X_test, y_train, y_test = train_test_split(X, y) # Train a logistic regression classifier logreg = LogisticRegression(C=1e9, solver=&#39;lbfgs&#39;, max_iter=1000) logreg.fit(X_train, y_train) # Build a confusion matrix y_true = y_test y_pred = logreg.predict(X_test) matrix = confusion_matrix(y_true, y_pred) label_names = pd.Series([&#39;unsuccessful&#39;, &#39;successful&#39;]) pd.DataFrame(matrix, columns=&#39;Predicted &#39; + label_names, index=&#39;Is &#39; + label_names) Predicted unsuccessful Predicted successful Is unsuccessful 1362 207 Is successful 285 420 That was disappointing! Even though we have all of those other columsn, it looks like high_success_rate_agency isn’t only the most important, but it’s the only thing we need. It feels like adding those other columns would only give us marginal improvement over just knowing whether we’re making a request to a high success rate agency. If you tell the algorithm “hey, I’m sending a FOIA to the EPA” it can completely ignore the actual content of the FOIA itself and tell you “oh sure yeah it’s gonna get granted.” This leads up to an important question, that focusing on what we’re even using this predictor for: if our classifier is only paying attention to the agency, is it helpful at all? "],
["leaving-out-our-best-feature.html", "6.1 Leaving out our best feature", " 6.1 Leaving out our best feature Even though high_success_rate_agency is a great feature in terms of predicting success or failure, it’s so powerful that it seems like cheating. We don’t want to hear that we’re being rejected just because we’re sending a FOIA to the CIA - we want to build the best predictor we can! What happens if we ignore the agency completely? Let’s remove high_success_rate_agency from our feaure set and see how our classifiers perform. We’re saying our current classifiers are useless, but are they really? It’s an open question! Knowing that we can be lazy with our request to the EPA, but need more effort when applying to the CIA might actually be useful. 6.1.1 Setting up our features # Remove high_success_rate_agency X = df[[ &#39;word_count&#39;, &#39;avg_sen_len&#39;, &#39;ref_foia&#39;, &#39;ref_fees&#39;, &#39;hyperlink&#39;, &#39;email_address&#39;, &#39;specificity&#39; ]] y = df.successful # Train/test split X_train, X_test, y_train, y_test = train_test_split(X, y) 6.1.2 K-nearest neighbors # Train a knn classifier knn = KNeighborsClassifier() knn.fit(X_train, y_train) # Build a confusion matrix y_true = y_test y_pred = knn.predict(X_test) matrix = confusion_matrix(y_true, y_pred) label_names = pd.Series([&#39;unsuccessful&#39;, &#39;successful&#39;]) pd.DataFrame(matrix, columns=&#39;Predicted &#39; + label_names, index=&#39;Is &#39; + label_names) Predicted unsuccessful Predicted successful Is unsuccessful 1319 271 Is successful 470 214 6.1.3 Logistic Regression # Train a logistic regression classifier logreg = LogisticRegression(C=1e9, solver=&#39;lbfgs&#39;, max_iter=1000) logreg.fit(X_train, y_train) # Build a confusion matrix y_true = y_test y_pred = logreg.predict(X_test) matrix = confusion_matrix(y_true, y_pred) label_names = pd.Series([&#39;unsuccessful&#39;, &#39;successful&#39;]) pd.DataFrame(matrix, columns=&#39;Predicted &#39; + label_names, index=&#39;Is &#39; + label_names) Predicted unsuccessful Predicted successful Is unsuccessful 1584 6 Is successful 679 5 6.1.4 Decision tree # Train a decision tree dec_tree = tree.DecisionTreeClassifier(max_depth=4) dec_tree.fit(X_train, y_train) # Build a confusion matrix y_true = y_test y_pred = dec_tree.predict(X_test) matrix = confusion_matrix(y_true, y_pred) label_names = pd.Series([&#39;unsuccessful&#39;, &#39;successful&#39;]) pd.DataFrame(matrix, columns=&#39;Predicted &#39; + label_names, index=&#39;Is &#39; + label_names) Predicted unsuccessful Predicted successful Is unsuccessful 1562 28 Is successful 660 24 6.1.5 Random forest # Train a decision tree forest = RandomForestClassifier(n_estimators=100) forest.fit(X_train, y_train) # Build a confusion matrix y_true = y_test y_pred = forest.predict(X_test) matrix = confusion_matrix(y_true, y_pred) label_names = pd.Series([&#39;unsuccessful&#39;, &#39;successful&#39;]) pd.DataFrame(matrix, columns=&#39;Predicted &#39; + label_names, index=&#39;Is &#39; + label_names) Predicted unsuccessful Predicted successful Is unsuccessful 1373 217 Is successful 474 210 6.1.6 Summary Was it shocking? Looking at our confusion matrices, it becomes clear that some algorithms relied heavily on high_success_rate_agency while others could be flexible once it was removed. The KNN did worse, but still got a decent number correct The logistic regression is almost as bad The decision tree does about as well as k-nearest neighbors The random forest performed slightly better than the KNN Random forests are generally thought of as a good “general purpose” classification algorithm. They’re flexible, they’re interpretable, and they usually work pretty well. We’ll see some problems with them in other chapters - they can be super slow, for example - but they get the job done. "],
["explaining-predictions.html", "7 Explaining predictions", " 7 Explaining predictions It’s good to know how an algorithm works, but it’s also useful to know how we arrived at a certain outcome. For example, why was this one granted and this one not granted? What can I do to improve my failing FOIA request? Unfortunately this is one of the most difficult parts of machine learning, especially depending on which algorithm you’re using. A logistic regression is by far the easiest, as you can read its output as “for each X increase in your average sentence length, your acceptance rate will go up by X.” If it works for your situation, go for it! But this simplistic approach is actually the same reason it did so poorly once we removed the high_success_rate_agency column, so that explainability might not be worth massive drop in quality that logistic regression brings to this problem. Our random forest performed very well, and can attempt to explain its predictions, but there’s a little gotcha hiding inside. eli5.explain_prediction_df(forest, X.iloc[0]) target feature weight value 0 &lt;BIAS&gt; 0.6979355 1.00000 0 avg_sen_len 0.0845278 21.55556 0 word_count 0.0655441 194.00000 0 ref_fees 0.0503816 1.00000 0 ref_foia 0.0278299 0.00000 0 specificity 0.0211641 8.00000 0 email_address -0.0020490 0.00000 0 hyperlink -0.0218991 0.00000 While it can tell you the features that were important to the decision, it isn’t as easy as “decrease your average sentence length,” as the forest is all of the interactions between them. For example, maybe using more words to get accross your point could be helpful, but if you add a hyperlink it’s better to be brief and let the URL do the explaining. Let’s look at a few more answers. eli5.explain_prediction_df(forest, X.iloc[1]) target feature weight value 1 avg_sen_len 0.3405992 9.5 1 &lt;BIAS&gt; 0.3020645 1.0 1 word_count 0.0679243 114.0 1 specificity 0.0612924 32.0 1 ref_fees 0.0131192 0.0 1 hyperlink 0.0103494 0.0 1 email_address -0.0021864 0.0 1 ref_foia -0.0131626 0.0 eli5.explain_prediction_df(forest, X.iloc[2]) target feature weight value 1 &lt;BIAS&gt; 0.3020645 1.0 1 avg_sen_len 0.1783382 39.5 1 word_count 0.1635550 79.0 1 specificity 0.1299959 9.0 1 ref_fees 0.0021674 0.0 1 email_address -0.0042799 0.0 1 ref_foia -0.0106619 0.0 1 hyperlink -0.0211791 1.0 It’s tough to explain that in a simple way! You might be able to guess and experiment based on what the important features are, but there’s always a chance the interactions are something you aren’t thinking of. "],
["percent-probability.html", "8 Percent probability", " 8 Percent probability So far we’ve only look at whether the classifier was correct or not, but that isn’t what the output of the FOIA Predictor is! When you run a request into the machine, it tells you the percent probability that you will be successful. predictions = forest.predict_proba([X.iloc[1]]) predictions[0] ## array([0.22, 0.78]) The code itself is built for more cases than just successful and unsuccessful - it gives you separate scores for each. The first one is the percent probability that your request will be denied, while the second one is the percent probability that your request will be successful. Since we only have two options, and you’re only interested in success, that’s all you really need to pay attention to. predictions = forest.predict_proba([X.iloc[1]]) predictions[0][1] ## 0.78 "],
["review.html", "9 Review", " 9 Review We took a look at the FOIA Predictor, a web site that allows you to enter your FOIA request and see what the chances are that it will be granted. We learned that this kind of a problem is an example of a classification. Classification problems involve a machine learning algorithm predicting a label or category (successful or denied) based on a set of details about the thing called features (average sentence length, destination agency, etc). Classification algorithms are tested in a way similar your a teacher giving out practice tests to study with. She knows the answers to all of the questions, just like we know whether each FOIA request was granted or not. We teach (or train) the algorithm on some of the FOIA requests we have, allowing it to figure out the difference between a successful and unsuccessful request. Once that’s done, we test it on the ones it hasn’t seen yet! The algorthm’s score in predicting is called accuracy. In this case it was easy to get a 70% score as long as you always guessed “rejected,” which means that accuracy is not the best measurement you can use when testing an algorithm. Instead, we turned to confusion matrices. A confusion matrix is a series of boxes that compare how the algorithm performed in more detail. It shows how many successful requests were predicted as either category, as well as for unsuccessful requests. We compared different kinds of classifiers with our dataset, and used the eli5 package to examine the reasons each classifier each came to its decisions. This concept is called explainability. The classifiers included k-nearest neighbors, logistic regression, decision tree, and random forest. K-nearest neighbors unfortunately couldn’t explain itself, but the others did a good job in listing what features they found to be important. It turned out that all classifiers were leaning heavily on the agency being applied to, so we tried removing that feature and running the algorithm again. Logistic regression failed completely without this information, but the others still did a fine job predicting. Finally, we looked at how to use eli5 to explain individual predictions. Because random forests can reflect complicated interactions we weren’t able to get advice on how to improve our FOIA requests, but we did get to see which features were the most important. "],
["discussion-topics.html", "10 Discussion topics", " 10 Discussion topics We don’t know how any of those classifiers work, but we’re still using them. How do we feel about that? When we removed the high_success_rate_agency column and performance plummeted for some of the classifiers and remained decent for others, it wasn’t something we were expecting at all. In the last question I called their performance “decent” - without high_success_rate_agency the random forest only predicted around 200 of 454 successfully. Is that really “decent,” or just better than 7 out of 454? If you were building a system like this, what kind of success rate would you like in your predictions before you release this to the world? Would it be different between predicting successful and unsuccessful requests? Which do you feel is more important in the FOIA Predictor: correctly predicting sucessful requests or predicting unsuccessful requests? Rachel said she was hoping other people would contribute to the FOIA Predictor to improve it, as it was released as an open-source project. Although it got some press and people did use it, no one seemed to have examined it or make their own contributions. Is sunlight really the best disinfectant if no one is looking at or commenting on published code? Do you think we should have left in high_success_rate_agency? While we could determine the features that were important to a classifier, we couldn’t turn those into actionable items. How useful or useless does that make our classifier? It’s easy to throw some garbage into the FOIA Predictor and get a high-percent-change result. Try mashing keys and see what happens! Does that mean the FOIA Predictor is useless? Let’s say the FOIA Predictor was much more accurate on both unsuccessful and successful requests - 80% or so. If putting entering garbage still predicted a successful request, does that mean the FOIA Predictor is useless? "]
]
